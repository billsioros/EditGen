{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "e70e6dbb-3211-4ef9-93f6-efaba764ac77"
   },
   "source": [
    "## Prepare the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "MODEL_NAME = \"facebook/musicgen-small\"\n",
    "\n",
    "if (Path(\"/\") / \"home\" / \"vsioros\" / \"data\").is_dir():\n",
    "    BASE_DIR = Path(\"/\") / \"home\" / \"vsioros\" / \"data\"\n",
    "    MODEL_NAME = \"facebook/musicgen-large\"\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "77ee39cc-654b-4f0e-b601-013e484c16f0"
   },
   "source": [
    "## Load the Model\n",
    "\n",
    "The pre-trained MusicGen small, medium and large checkpoints can be loaded from the [pre-trained weights](https://huggingface.co/models?search=facebook/musicgen-) on the Hugging Face Hub. Change the repo id with the checkpoint size you wish to load. We'll default to the small checkpoint, which is the fastest of the three but has the lowest audio quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "id": "b0d87424-9f38-4658-ba47-2a465d52ad77"
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from numpy.typing import NDArray\n",
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "\n",
    "\n",
    "class ModelProxy:\n",
    "    def __init__(self, model_name: str, guidance_scale: float = 3.0):\n",
    "        self.model_name = model_name\n",
    "        self.guidance_scale = guidance_scale\n",
    "\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.processor = AutoProcessor.from_pretrained(model_name)\n",
    "        self.model = MusicgenForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "\n",
    "    def generate(self, inputs: dict[str, Any], max_new_tokens: int = 512) -> NDArray[np.float64]:\n",
    "        return (\n",
    "            self.model.generate(\n",
    "                **inputs.to(self.device),\n",
    "                do_sample=True,\n",
    "                guidance_scale=self.guidance_scale,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "            )\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "            .squeeze()\n",
    "        )\n",
    "\n",
    "    def encode(self, prompts: list[str]) -> dict[str, Any]:\n",
    "        return self.processor(text=prompts, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    def decode(self, encoded_token: NDArray[np.float64]) -> str:\n",
    "        return self.processor.decode(encoded_token)\n",
    "\n",
    "    @property\n",
    "    def sampling_rate(self) -> float:\n",
    "        return self.model.config.audio_encoder.sampling_rate\n",
    "\n",
    "    @property\n",
    "    def frame_rate(self) -> float:\n",
    "        return self.model.config.audio_encoder.frame_rate\n",
    "\n",
    "    @property\n",
    "    def decoder_layers(self) -> list[torch.nn.Module]:\n",
    "        return self.model.decoder.model.decoder.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_proxy = ModelProxy(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Prompt-to-Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers.utils import (\n",
    "    logging,\n",
    ")\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "def register_attention_control(model_proxy, controller):\n",
    "    def ca_forward(self, attention_type):\n",
    "        def forward(\n",
    "            hidden_states: torch.Tensor,\n",
    "            key_value_states: Optional[torch.Tensor] = None,\n",
    "            past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            layer_head_mask: Optional[torch.Tensor] = None,\n",
    "            output_attentions: bool = False,\n",
    "        ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "            \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "            # if key_value_states are provided this layer is used as a cross-attention layer\n",
    "            # for the decoder\n",
    "            is_cross_attention = key_value_states is not None\n",
    "\n",
    "            bsz, tgt_len, _ = hidden_states.size()\n",
    "\n",
    "            # get query proj\n",
    "            query_states = self.q_proj(hidden_states) * self.scaling\n",
    "            # get key, value proj\n",
    "            # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n",
    "            # is checking that the `sequence_length` of the `past_key_value` is the same as\n",
    "            # the provided `key_value_states` to support prefix tuning\n",
    "            if (\n",
    "                is_cross_attention\n",
    "                and past_key_value is not None\n",
    "                and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
    "            ):\n",
    "                # reuse k,v, cross_attentions\n",
    "                key_states = past_key_value[0]\n",
    "                value_states = past_key_value[1]\n",
    "            elif is_cross_attention:\n",
    "                # cross_attentions\n",
    "                key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
    "                value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
    "            elif past_key_value is not None:\n",
    "                # reuse k, v, self_attention\n",
    "                key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "                value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "                key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
    "                value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
    "            else:\n",
    "                # self_attention\n",
    "                key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "                value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "            if self.is_decoder:\n",
    "                # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "                # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "                # key/value_states (first \"if\" case)\n",
    "                # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "                # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "                # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "                # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "                past_key_value = (key_states, value_states)\n",
    "\n",
    "            proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "            query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "            key_states = key_states.reshape(*proj_shape)\n",
    "            value_states = value_states.reshape(*proj_shape)\n",
    "\n",
    "            src_len = key_states.size(1)\n",
    "            attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
    "\n",
    "            if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
    "                    f\" {attn_weights.size()}\",\n",
    "                )\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                    raise ValueError(\n",
    "                        f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\",\n",
    "                    )\n",
    "                attn_weights = (\n",
    "                    attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
    "                )\n",
    "                attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "            attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "            attn_weights = controller(attn_weights, is_cross_attention, attention_type)\n",
    "\n",
    "            if layer_head_mask is not None:\n",
    "                if layer_head_mask.size() != (self.num_heads,):\n",
    "                    raise ValueError(\n",
    "                        f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n",
    "                        f\" {layer_head_mask.size()}\",\n",
    "                    )\n",
    "                attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(\n",
    "                    bsz,\n",
    "                    self.num_heads,\n",
    "                    tgt_len,\n",
    "                    src_len,\n",
    "                )\n",
    "                attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "            if output_attentions:\n",
    "                # this operation is a bit awkward, but it's required to\n",
    "                # make sure that attn_weights keeps its gradient.\n",
    "                # In order to do so, attn_weights have to be reshaped\n",
    "                # twice and have to be reused in the following\n",
    "                attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "                attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "            else:\n",
    "                attn_weights_reshaped = None\n",
    "\n",
    "            attn_probs = nn.functional.dropout(\n",
    "                attn_weights,\n",
    "                p=self.dropout,\n",
    "                training=self.training,\n",
    "            )\n",
    "\n",
    "            attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "            if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "                raise ValueError(\n",
    "                    f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
    "                    f\" {attn_output.size()}\",\n",
    "                )\n",
    "\n",
    "            attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "            attn_output = attn_output.transpose(1, 2)\n",
    "\n",
    "            # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n",
    "            # partitioned across GPUs when using tensor-parallelism.\n",
    "            attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
    "\n",
    "            attn_output = self.out_proj(attn_output)\n",
    "\n",
    "            return attn_output, attn_weights_reshaped, past_key_value\n",
    "\n",
    "        return forward\n",
    "\n",
    "    def register_recr(net_, count, attention_type):\n",
    "        if net_.__class__.__name__ == \"MusicgenAttention\":\n",
    "            net_.forward = ca_forward(net_, attention_type)\n",
    "            return count + 1\n",
    "\n",
    "        return count\n",
    "\n",
    "    cross_att_count = 0\n",
    "    sub_nets = model_proxy.decoder_layers.named_children()\n",
    "    for net in sub_nets:\n",
    "        if net[1].__class__.__name__ != \"MusicgenDecoderLayer\":\n",
    "            continue\n",
    "\n",
    "        for subnet in net[1].named_children():\n",
    "            attention_type = None\n",
    "            if subnet[0] == \"encoder_attn\":\n",
    "                attention_type = \"cross\"\n",
    "            elif subnet[0] == \"self_attn\":\n",
    "                attention_type = \"self\"\n",
    "\n",
    "            if attention_type is not None:\n",
    "                cross_att_count += register_recr(subnet[1], 0, attention_type)\n",
    "\n",
    "    controller.num_att_layers = cross_att_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class BaseController(ABC):\n",
    "    def reset(self):\n",
    "        self.num_att_layers = 0\n",
    "        self.batch_size = -1\n",
    "        self.max_new_tokens = -1\n",
    "\n",
    "        self.cur_att_layer = 0\n",
    "        self.cur_step = 0\n",
    "\n",
    "    def __call__(self, attn_weights, is_cross, attention_type) -> None:\n",
    "        self.cur_att_layer = self.cur_att_layer + 1\n",
    "        if self.cur_att_layer == self.num_att_layers:\n",
    "            self.cur_att_layer = 0\n",
    "            self.cur_step = self.cur_step + 1\n",
    "\n",
    "        # Exclude unconditional inputs\n",
    "        h1 = attn_weights.shape[0] // 2\n",
    "        attn = attn_weights[:h1]\n",
    "\n",
    "        # Reshape according to batch size\n",
    "        h2 = attn.shape[0] // (self.batch_size)\n",
    "\n",
    "        attn = attn.reshape(self.batch_size, h2, *attn.shape[1:])\n",
    "\n",
    "        if is_cross:\n",
    "            attn = self.replace_cross_attention(attn, is_cross, attention_type)\n",
    "            # attn /= torch.sum(attn, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            attn = self.replace_self_attention(attn)\n",
    "\n",
    "        attn = attn.reshape(self.batch_size * h2, *attn.shape[2:])\n",
    "\n",
    "        attn_weights[:h1] = attn\n",
    "\n",
    "        return attn_weights\n",
    "\n",
    "    @abstractmethod\n",
    "    def replace_self_attention(self, attn):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class EmptyController(BaseController):\n",
    "    def replace_self_attention(self, attn):\n",
    "        return attn\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type):\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class AttentionStore(BaseController):\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "\n",
    "        self.features = defaultdict(list)\n",
    "\n",
    "    def get_self_attention(self):\n",
    "        tensors = self.features[\"self\"]\n",
    "        tensors = [tensor.mean(dim=-1) for tensor in tensors]\n",
    "        tensors = torch.stack(tensors)\n",
    "        tensors = tensors.view(self.max_new_tokens, -1, *tensors.shape[1:])\n",
    "\n",
    "        return tensors\n",
    "\n",
    "    def get_cross_attention(self):\n",
    "        tensors = self.features[\"cross\"]\n",
    "        tensors = torch.stack(tensors)\n",
    "        tensors = tensors.view(self.max_new_tokens, -1, *tensors[0].shape)\n",
    "\n",
    "        return tensors\n",
    "\n",
    "    def get_self_attention_importance(self):\n",
    "        aggregate_cross_attention = self.get_self_attention()\n",
    "        aggregate_cross_attention = aggregate_cross_attention[:, :, 1:, :, :]\n",
    "        aggregate_cross_attention = aggregate_cross_attention.mean(dim=(0, 3, 4))\n",
    "\n",
    "        # Min-Max scaling to normalize values between 0 and 1 for each column (sample)\n",
    "        min_values = aggregate_cross_attention.min(dim=0).values\n",
    "        max_values = aggregate_cross_attention.max(dim=0).values\n",
    "\n",
    "        normalized_scores = (aggregate_cross_attention - min_values) / (max_values - min_values)\n",
    "\n",
    "        # Get indices that would sort the layers based on their mean scores\n",
    "        sorted_indices = torch.argsort(normalized_scores, descending=True, dim=0)\n",
    "        sorted_indices = sorted_indices.view(sorted_indices.shape[1], -1)\n",
    "\n",
    "        # self-attention layers are called first and thusly hold indices 1, 3, 5 etc.\n",
    "        sorted_indices = 2 * sorted_indices + 1\n",
    "\n",
    "        return sorted_indices\n",
    "\n",
    "    def get_cross_attention_importance(self, word_piece_index):\n",
    "        aggregate_cross_attention = self.get_cross_attention()\n",
    "        aggregate_cross_attention = aggregate_cross_attention[:, :, 1:, :, :, word_piece_index]\n",
    "        aggregate_cross_attention = aggregate_cross_attention.mean(dim=(0, 3, 4))\n",
    "\n",
    "        # Min-Max scaling to normalize values between 0 and 1 for each column (sample)\n",
    "        min_values = aggregate_cross_attention.min(dim=0).values\n",
    "        max_values = aggregate_cross_attention.max(dim=0).values\n",
    "\n",
    "        normalized_scores = (aggregate_cross_attention - min_values) / (max_values - min_values)\n",
    "\n",
    "        # Get indices that would sort the layers based on their mean scores\n",
    "        sorted_indices = torch.argsort(normalized_scores, descending=True, dim=0)\n",
    "        sorted_indices = sorted_indices.view(sorted_indices.shape[1], -1)\n",
    "\n",
    "        # cross-attention layers are called second and thusly hold indices 2, 4, 6 etc.\n",
    "        sorted_indices = 2 * (sorted_indices + 1)\n",
    "\n",
    "        return sorted_indices\n",
    "\n",
    "    def get_aggregate_cross_attention(self):\n",
    "        return torch.mean(torch.stack(self.features[\"cross\"]), axis=0)\n",
    "\n",
    "    def replace_self_attention(self, attn) -> None:\n",
    "        self.features[\"self\"].append(attn)\n",
    "\n",
    "        return attn\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type) -> None:\n",
    "        self.features[\"cross\"].append(attn_weights)\n",
    "\n",
    "        return attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEditController(BaseController):\n",
    "    def replace_self_attention(self, attn):\n",
    "        attn_base, att_replace = attn[0], attn[1:]\n",
    "\n",
    "        return attn_base.unsqueeze(0).expand(att_replace.shape[0] + 1, *attn_base.shape)\n",
    "\n",
    "\n",
    "class RandomController(BaseEditController):\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type):\n",
    "        attn_weights[1:] = torch.randn_like(attn_weights[0])\n",
    "\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class IgnoreWordController(BaseEditController):\n",
    "    def __init__(self, indices: list[int]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.indices = indices\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type):\n",
    "        attn_weights[1:, :, :, self.indices] = 0\n",
    "\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class ReplaceWordController(BaseEditController):\n",
    "    def __init__(self, indices: list[list[int]], blend: float = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.source_indices, self.target_indices = indices\n",
    "        self.blend = blend\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type):\n",
    "        source_attn = attn_weights[0, :, :, self.source_indices]\n",
    "        averaged_attn = source_attn.mean(dim=-1, keepdims=True)\n",
    "\n",
    "        # Repeat averaged attention values to match dimensions of the target attention\n",
    "        averaged_attn_repeated = averaged_attn.expand(-1, -1, len(self.target_indices))\n",
    "\n",
    "        attn_weights[1:, :, :, self.target_indices] = (1 - self.blend) * attn_weights[\n",
    "            1:,\n",
    "            :,\n",
    "            :,\n",
    "            self.target_indices,\n",
    "        ] + self.blend * averaged_attn_repeated\n",
    "\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class RefineController(BaseEditController):\n",
    "    def __init__(self, indices: list[list[int]], blend: float = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.source_indices, self.target_indices = indices\n",
    "        self.blend = blend\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type):\n",
    "        attn_weights[1:, :, :, self.target_indices] = (1 - self.blend) * attn_weights[\n",
    "            1:,\n",
    "            :,\n",
    "            :,\n",
    "            self.target_indices,\n",
    "        ] + self.blend * attn_weights[0, :, :, self.source_indices]\n",
    "\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class ReweightWordController(BaseEditController):\n",
    "    def __init__(self, indices: list[int], weight: float = 5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.indices = indices\n",
    "        self.weight = weight\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type):\n",
    "        attn_weights[1:] = attn_weights[0]\n",
    "\n",
    "        non_target_indices = [i for i in range(attn_weights.shape[-1]) if i not in self.indices]\n",
    "        attn_weights[1:, :, :, non_target_indices] /= self.weight\n",
    "\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class ReplaceController(BaseEditController):\n",
    "    def __init__(self, blend: float = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blend = blend\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type):\n",
    "        attn_weights[1:] = (1 - self.blend) * attn_weights[1:] + self.blend * attn_weights[0]\n",
    "\n",
    "        return attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class ControllerModifier(BaseController):\n",
    "    def __init__(self, controller: BaseController) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.controller = controller\n",
    "\n",
    "    def __getattr__(self, name: str):\n",
    "        if name == \"controller\":\n",
    "            return super().__getattr__(name)\n",
    "\n",
    "        return getattr(self.controller, name)\n",
    "\n",
    "    def __setattr__(self, name: str, value: Any):\n",
    "        if name == \"controller\":\n",
    "            return super().__setattr__(name, value)\n",
    "\n",
    "        return setattr(self.controller, name, value)\n",
    "\n",
    "\n",
    "class OffsetControllerModifier(ControllerModifier):\n",
    "    def __init__(self, controller: BaseController, offset: float = 0.0) -> None:\n",
    "        super().__init__(controller)\n",
    "\n",
    "        assert 0.0 <= offset <= 1.0\n",
    "\n",
    "        self.offset = offset\n",
    "\n",
    "    def replace_self_attention(self, attn):\n",
    "        if self.cur_step < round(self.offset * self.max_new_tokens):\n",
    "            return attn\n",
    "\n",
    "        return self.controller.replace_self_attention(attn)\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type):\n",
    "        if self.cur_step < round(self.offset * self.max_new_tokens):\n",
    "            return attn_weights\n",
    "\n",
    "        return self.controller.replace_cross_attention(attn_weights, is_cross, attention_type)\n",
    "\n",
    "\n",
    "class AttentionHeadControllerModifier(ControllerModifier):\n",
    "    def __init__(self, controller: BaseController, attention_head_indices: list[int]) -> None:\n",
    "        super().__init__(controller)\n",
    "\n",
    "        self.attention_head_indices = attention_head_indices\n",
    "\n",
    "    def replace_self_attention(self, attn):\n",
    "        return self.controller.replace_self_attention(attn)\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type):\n",
    "        attn_weights_slice = attn_weights[:, self.attention_head_indices, :, :]\n",
    "        attn_weights_slice = self.controller.replace_cross_attention(\n",
    "            attn_weights_slice,\n",
    "            is_cross,\n",
    "            attention_type,\n",
    "        )\n",
    "\n",
    "        attn_weights[:, self.attention_head_indices, :, :] = attn_weights_slice\n",
    "\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class SelfAttentionLerpControllerModifier(ControllerModifier):\n",
    "    def __init__(self, controller: BaseController) -> None:\n",
    "        super().__init__(controller)\n",
    "\n",
    "    def replace_self_attention(self, attn):\n",
    "        blend = self.cur_att_layer / self.num_att_layers\n",
    "\n",
    "        attn[1:] = (1 - blend) * attn[1:] + blend * attn[0]\n",
    "\n",
    "        return attn\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type) -> None:\n",
    "        return self.controller.replace_cross_attention(attn_weights, is_cross, attention_type)\n",
    "\n",
    "\n",
    "class SelfAttentionCutoffControllerModifier(ControllerModifier):\n",
    "    def __init__(self, controller: BaseController, threshold: float = 0.75) -> None:\n",
    "        super().__init__(controller)\n",
    "\n",
    "        assert 0.0 <= threshold <= 1.0\n",
    "\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def replace_self_attention(self, attn):\n",
    "        if self.cur_att_layer <= np.floor(self.threshold * self.num_att_layers):\n",
    "            return self.controller.replace_self_attention(attn)\n",
    "\n",
    "        return attn\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type) -> None:\n",
    "        return self.controller.replace_cross_attention(attn_weights, is_cross, attention_type)\n",
    "\n",
    "\n",
    "class AttentionLerpControllerModifier(ControllerModifier):\n",
    "    def __init__(self, controller: BaseController) -> None:\n",
    "        super().__init__(controller)\n",
    "\n",
    "    def replace_self_attention(self, attn):\n",
    "        blend = self.cur_att_layer / self.num_att_layers\n",
    "\n",
    "        attn = (1 - blend) * attn + blend * self.controller.replace_self_attention(attn)\n",
    "\n",
    "        return attn\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type) -> None:\n",
    "        blend = self.cur_att_layer / self.num_att_layers\n",
    "\n",
    "        attn_weights = (\n",
    "            1 - blend\n",
    "        ) * attn_weights + blend * self.controller.replace_cross_attention(\n",
    "            attn_weights.clone(),\n",
    "            is_cross,\n",
    "            attention_type,\n",
    "        )\n",
    "\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class AttentionCutoffControllerModifier(ControllerModifier):\n",
    "    def __init__(self, controller: BaseController, threshold: float = 0.75) -> None:\n",
    "        super().__init__(controller)\n",
    "\n",
    "        assert 0.0 <= threshold <= 1.0\n",
    "\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def replace_self_attention(self, attn):\n",
    "        if self.cur_att_layer <= np.floor(self.threshold * self.num_att_layers):\n",
    "            return self.controller.replace_self_attention(attn)\n",
    "\n",
    "        return attn\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type) -> None:\n",
    "        if self.cur_att_layer <= np.floor(self.threshold * self.num_att_layers):\n",
    "            return self.controller.replace_cross_attention(attn_weights, is_cross, attention_type)\n",
    "\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class DecoderLayerControllerModifier(ControllerModifier):\n",
    "    def __init__(self, controller: BaseController, decoder_layer_indices: set[int]) -> None:\n",
    "        super().__init__(controller)\n",
    "\n",
    "        self.decoder_layer_indices = decoder_layer_indices\n",
    "\n",
    "    def replace_self_attention(self, attn):\n",
    "        if self.cur_att_layer in self.decoder_layer_indices:\n",
    "            return self.controller.replace_self_attention(attn)\n",
    "\n",
    "        return attn\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type):\n",
    "        if self.cur_att_layer in self.decoder_layer_indices:\n",
    "            return self.controller.replace_cross_attention(attn_weights, is_cross, attention_type)\n",
    "\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class StrengthControllerModifier(ControllerModifier):\n",
    "    def __init__(self, controller: BaseController, strength: float = 1.0) -> None:\n",
    "        super().__init__(controller)\n",
    "\n",
    "        self.strength = strength\n",
    "\n",
    "    def replace_self_attention(self, attn):\n",
    "        attn = (1 - self.strength) * attn + self.strength * self.controller.replace_self_attention(\n",
    "            attn.clone(),\n",
    "        )\n",
    "\n",
    "        return attn\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type) -> None:\n",
    "        attn_weights = (\n",
    "            1 - self.strength\n",
    "        ) * attn_weights + self.strength * self.controller.replace_cross_attention(\n",
    "            attn_weights.clone(),\n",
    "            is_cross,\n",
    "            attention_type,\n",
    "        )\n",
    "\n",
    "        return attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "CONTROLLER_REG = re.compile(r\"Controller.*\")\n",
    "SNAKE_CASE_REG = re.compile(r\"(?<!^)(?=[A-Z])\")\n",
    "\n",
    "RESULTS_DIR = BASE_DIR / \"results\" / \"musicgen\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def _get_controller_subpath(controller: BaseController) -> Path:\n",
    "    controller_name = CONTROLLER_REG.sub(\"\", controller.__class__.__name__)\n",
    "    controller_name = SNAKE_CASE_REG.sub(\"_\", controller_name).lower()\n",
    "\n",
    "    return Path(controller_name)\n",
    "\n",
    "\n",
    "def get_controller_subpath(controller: BaseController) -> Path:\n",
    "    if isinstance(controller, DecoderLayerControllerModifier):\n",
    "        return (\n",
    "            _get_controller_subpath(controller)\n",
    "            / f\"{''.join(map(lambda x: f'{x:02d}', controller.decoder_layer_indices))}\"\n",
    "            / get_controller_subpath(controller.controller)\n",
    "        )\n",
    "    elif isinstance(controller, ReplaceController):\n",
    "        return _get_controller_subpath(controller) / f\"{controller.blend:.2f}\"\n",
    "    elif isinstance(controller, ReplaceWordController):\n",
    "        return (\n",
    "            _get_controller_subpath(controller)\n",
    "            / f\"{controller.index:02d}\"\n",
    "            / f\"{controller.blend:.2f}\"\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(controller.__class__.__name__)\n",
    "\n",
    "\n",
    "def construct_output_folder_path(controller: BaseController) -> Path:\n",
    "    return RESULTS_DIR / get_controller_subpath(controller) / datetime.now().strftime(\"%Y_%m_%d\")\n",
    "\n",
    "\n",
    "def get_tokens(prompts: list[str]):\n",
    "    inputs = model_proxy.encode(prompts)\n",
    "\n",
    "    tokens = []\n",
    "    for index in range(len(prompts)):\n",
    "        tokens.append([model_proxy.decode(item) for item in inputs[\"input_ids\"][index]])\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_replacement_indices(prompts: list[str], word_a: str, word_b: str) -> list[list[int]]:\n",
    "    token_groups = get_tokens(prompts)\n",
    "\n",
    "    if len(prompts[0].split()) != len(prompts[1].split()):\n",
    "        raise NotImplementedError(f\"Different prompt lengths ({prompts})\")\n",
    "\n",
    "    indices = []\n",
    "    for word, tokens in zip([word_a, word_b], token_groups):\n",
    "        prompt_indices, substring = [], []\n",
    "        for i in range(len(tokens)):\n",
    "            if word.startswith(\"\".join([*substring, tokens[i]])):\n",
    "                substring.append(tokens[i])\n",
    "                prompt_indices.append(i)\n",
    "        indices.append(prompt_indices)\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "def get_reweight_word_indices(prompts: list[str], word: str) -> tuple[list[str], list[int]]:\n",
    "    return get_replacement_indices(prompts, word, word)[0]\n",
    "\n",
    "\n",
    "def get_refine_word_indices(prompts: list[str]) -> tuple[list[str], list[int]]:\n",
    "    token_groups = get_tokens(prompts)\n",
    "\n",
    "    indices = []\n",
    "    for i, token_i in enumerate(token_groups[0]):\n",
    "        for j, token_j in enumerate(token_groups[1]):\n",
    "            if token_i.startswith(\"<\") or token_j.startswith(\"<\"):\n",
    "                continue\n",
    "\n",
    "            if token_i == token_j:\n",
    "                indices.append((i, j))\n",
    "\n",
    "    return list(zip(*indices))\n",
    "\n",
    "\n",
    "def get_ignore_indices(prompts: list[str]) -> tuple[list[str], list[int]]:\n",
    "    tokens_a, tokens_b = prompts[0].split(), prompts[1].split()\n",
    "    if len(tokens_a) != len(tokens_b):\n",
    "        raise NotImplementedError(\"Different prompt lengths\")\n",
    "\n",
    "    index = tokens_b.index(\"<IGNORE>\")\n",
    "    word = tokens_a[index]\n",
    "\n",
    "    return [prompts[0], prompts[0]], get_replacement_indices([prompts[0], prompts[0]], word, word)[\n",
    "        0\n",
    "    ]\n",
    "\n",
    "\n",
    "def run_and_display(prompts, output_folder, controller=None, seed=0, audio_length=10.0):\n",
    "    max_new_tokens = 2 ** round(np.log2(audio_length * model_proxy.frame_rate))\n",
    "\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    if controller is None:\n",
    "        controller = EmptyController()\n",
    "\n",
    "    controller.reset()\n",
    "    controller.batch_size = len(prompts)\n",
    "    controller.max_new_tokens = max_new_tokens\n",
    "\n",
    "    register_attention_control(model_proxy, controller)\n",
    "\n",
    "    inputs = model_proxy.encode(prompts)\n",
    "\n",
    "    return model_proxy.generate(inputs, max_new_tokens=max_new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### ISMIR Evaluation\n",
    "\n",
    "We incorporate different audio editing axes:\n",
    "\n",
    "- Instrument Replacement: In these prompts, one instrument or sound source is replaced with another instrument or sound. For example, replacing blues ensemble with guitar and drums with country ensemble with guitar and drums, or replacing acoustic guitar solo with electric guitar solo.\n",
    "\n",
    "- Mood/Tonal Change: These prompts involve changing the mood or tonality of the music. For instance, transforming a happy violin solo into a sad violin solo, or converting a major chord pop song into a minor chord pop song.\n",
    "\n",
    "- Genre Shift: These prompts involve shifting the genre or style of the music. For example, transitioning from a rock riff on electric guitar to a metal riff on electric guitar, or changing a jazz beat with saxophone into a hip-hop beat with saxophone.\n",
    "\n",
    "- Melodic Transformation: These edits involve altering the melodic content of the music. This can include changes in melodic contour, intervals, motifs, and melodies. For example, transforming a melodic line from ascending to descending or changing the melodic intervals to create a different melodic feel.\n",
    "\n",
    "- Harmonic Modification: These edits involve modifying the harmonic structure of the music. This can include changes in chord progressions, harmonic rhythm, harmonic density, and harmonic tension. For instance, altering the chord progression from a standard I-IV-V to a more complex progression or introducing chromaticism to the harmony.\n",
    "\n",
    "- Form/Structure Variation: These edits involve variations in the overall form or structure of the music. This can include changes in sectional arrangement, repetitions, transitions, and developmental processes. For example, restructuring a piece by adding or removing sections, or altering the order of musical events to create a different narrative flow.\n",
    "\n",
    "We generate a plethora of prompt pairs utilizing ChatGPT and the accompanying text-prompt:\n",
    "\n",
    "#### Replace\n",
    "\n",
    "```txt\n",
    "Generate a list of quadruples containing:\n",
    "1. The edit category,\n",
    "2. The original word in the prompt,\n",
    "3. The replacement word,\n",
    "4. The prompt pair consisting of the source and edited prompt. \n",
    "\n",
    "Ensure to include at least one entry for each of the following edit categories and output only a single Python list.\n",
    "\n",
    "Example entry:\n",
    "(\n",
    "   \"Genre Shift\",\n",
    "   \"blues\",\n",
    "   \"country\",\n",
    "   (\"blues ensemble with guitar and drums\", \"country ensemble with guitar and drums\"),\n",
    "),\n",
    "\n",
    "Edit Categories:\n",
    "\n",
    "- Instrument Replacement: Replace one instrument or sound source with another.\n",
    "- Mood/Tonal Change: Change the mood or tonality of the music.\n",
    "- Genre Shift: Shift the genre or style of the music.\n",
    "- Melodic Transformation: Alter the melodic content of the music.\n",
    "- Harmonic Modification: Modify the harmonic structure of the music.\n",
    "- Form/Structure Variation: Vary the overall form or structure of the music.\n",
    "```\n",
    "\n",
    "#### Refine\n",
    "\n",
    "```txt\n",
    "Generate a list of tuples containing:\n",
    "1. The edit category,\n",
    "2. The prompt pair consisting of the source and edited prompt. \n",
    "\n",
    "The source prompt should strictly be a substring of the edited prompt. The edited prompt should only add details.\n",
    "\n",
    "Ensure to include at least one entry for each of the following edit categories and output only a single Python list.\n",
    "\n",
    "Example entry:\n",
    "(\"piano melody\", \"jazz piano melody with improvisation\"),\n",
    "\n",
    "Edit Categories:\n",
    "\n",
    "- Instrument Enhancement: Enhancing the sound by adding effects or layering additional sounds without replacing the instrument.\n",
    "- Mood/Tonal Enhancement: Modifying tonality or mood through adjustments like reverb or EQ settings.\n",
    "- Genre Fusion: Combining elements from different genres while preserving the original essence.\n",
    "- Melodic Embellishment: Adding ornamentations or variations to enrich the melody's expressiveness.\n",
    "- Harmonic Enrichment: Enriching the harmonic structure by adding chords or layers for a fuller sound.\n",
    "- Form/Structure Expansion: Elaborating on the form or structure by adding new sections or transitions for complexity.\n",
    "```\n",
    "\n",
    "### Reweight\n",
    "\n",
    "```txt\n",
    "Generate a list of tuples containing:\n",
    "1. The edit category,\n",
    "2. The target word\n",
    "2. The prompt pair consisting of the source and edited prompt. \n",
    "\n",
    "The source prompt and the edited prompt should be identical. The target word must be included in both prompts, indicating which aspect of the described audio should be intensified or diminished.\n",
    "\n",
    "Ensure to include at least one entry for each of the following edit categories and output only a single Python list.\n",
    "\n",
    "Example entry:\n",
    "(\"happy\", (\"happy acoustic guitar solo\", \"happy acoustic guitar solo\"))\n",
    "Edit Categories:\n",
    "\n",
    "Instrument Reinforcement: Enhancing the sound of specific instruments or adding additional layers to enrich the overall texture without replacing them entirely.\n",
    "Mood/Tonal Brightening: Adjusting tonality or mood through changes like increasing brightness or introducing uplifting effects to evoke a happier atmosphere.\n",
    "Genre Blend: Mixing elements from various genres while maintaining the song's core identity to create a fusion that embodies a happier vibe.\n",
    "Melodic Flourish: Introducing embellishments or variations in the melody to make it more lively, optimistic, and joyful.\n",
    "Harmonic Enlivening: Enriching the harmonic structure by adding chords or harmonic layers that convey a sense of positivity and energy.\n",
    "Form/Structure Expansion: Expanding the song's structure or adding new sections to build anticipation, create contrasts, and enhance the overall uplifting mood.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_group = [\n",
    "    (\n",
    "        \"Instrument Replacement\",\n",
    "        \"violin\",\n",
    "        \"cello\",\n",
    "        (\"beautiful violin solo\", \"beautiful cello solo\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Mood/Tonal Change\",\n",
    "        \"happy\",\n",
    "        \"sad\",\n",
    "        (\"uplifting piano melody\", \"melancholic piano melody\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Genre Shift\",\n",
    "        \"rock\",\n",
    "        \"metal\",\n",
    "        (\"rock riff on electric guitar\", \"metal riff on electric guitar\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Melodic Transformation\",\n",
    "        \"ascending\",\n",
    "        \"descending\",\n",
    "        (\"ascending melodic line\", \"descending melodic line\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Harmonic Modification\",\n",
    "        \"I-IV-V\",\n",
    "        \"ii-V-I\",\n",
    "        (\"standard chord progression\", \"jazzier chord progression\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Form/Structure Variation\",\n",
    "        \"adding\",\n",
    "        \"removing\",\n",
    "        (\n",
    "            \"restructuring a piece by adding sections\",\n",
    "            \"restructuring a piece by removing sections\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Instrument Replacement\",\n",
    "        \"guitar\",\n",
    "        \"piano\",\n",
    "        (\"guitar solo\", \"piano solo\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Mood/Tonal Change\",\n",
    "        \"major\",\n",
    "        \"minor\",\n",
    "        (\"major chord pop song\", \"minor chord pop song\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Genre Shift\",\n",
    "        \"jazz\",\n",
    "        \"hip-hop\",\n",
    "        (\"jazz beat with saxophone\", \"hip-hop beat with saxophone\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Melodic Transformation\",\n",
    "        \"motif\",\n",
    "        \"variation\",\n",
    "        (\"repeating motif\", \"varied motif\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Harmonic Modification\",\n",
    "        \"standard\",\n",
    "        \"chromatic\",\n",
    "        (\"standard chord progression\", \"chromatic chord progression\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Form/Structure Variation\",\n",
    "        \"repetitions\",\n",
    "        \"transitions\",\n",
    "        (\"repeating sections\", \"transitional sections\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Instrument Replacement\",\n",
    "        \"drums\",\n",
    "        \"synthesizer\",\n",
    "        (\"drum solo\", \"synthesizer solo\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Mood/Tonal Change\",\n",
    "        \"dark\",\n",
    "        \"ethereal\",\n",
    "        (\"dark ambient track\", \"ethereal ambient track\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Genre Shift\",\n",
    "        \"pop\",\n",
    "        \"reggae\",\n",
    "        (\"pop song with catchy hooks\", \"reggae song with catchy hooks\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Melodic Transformation\",\n",
    "        \"intervals\",\n",
    "        \"sequences\",\n",
    "        (\"melodic intervals\", \"melodic sequences\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Harmonic Modification\",\n",
    "        \"I-vi-IV-V\",\n",
    "        \"ii-V-I\",\n",
    "        (\"typical chord progression\", \"jazzier chord progression\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Form/Structure Variation\",\n",
    "        \"intro\",\n",
    "        \"outro\",\n",
    "        (\"introductory section\", \"concluding section\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Instrument Replacement\",\n",
    "        \"trumpet\",\n",
    "        \"flute\",\n",
    "        (\"trumpet solo\", \"flute solo\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Mood/Tonal Change\",\n",
    "        \"uplifting\",\n",
    "        \"haunting\",\n",
    "        (\"uplifting guitar melody\", \"haunting guitar melody\"),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_group = [\n",
    "    (\n",
    "        \"Instrument Enhancement\",\n",
    "        (\"piano melody\", \"jazz piano melody with added chorus effect for depth and warmth\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Mood/Tonal Enhancement\",\n",
    "        (\"guitar riff\", \"ethereal guitar riff with shimmering reverb and atmospheric delay\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Genre Fusion\",\n",
    "        (\n",
    "            \"hip-hop beat\",\n",
    "            \"trap-infused hip-hop beat with electronic synth arpeggios and 808 bass\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Melodic Embellishment\",\n",
    "        (\n",
    "            \"vocal line\",\n",
    "            \"soulful vocal line with intricate melismatic runs and emotive vibrato\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Harmonic Enrichment\",\n",
    "        (\n",
    "            \"chord progression\",\n",
    "            \"lush chord progression with added ninth and eleventh extensions for richness\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Form/Structure Expansion\",\n",
    "        (\n",
    "            \"bridge section\",\n",
    "            \"extended bridge section with modulating key centers and layered counterpoint\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Instrument Enhancement\",\n",
    "        (\n",
    "            \"drum groove\",\n",
    "            \"dynamic drum groove with layered percussion and enhanced stereo imaging\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Mood/Tonal Enhancement\",\n",
    "        (\n",
    "            \"ambient pad\",\n",
    "            \"serene ambient pad with subtle modulated filters and soft side-chain compression\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Genre Fusion\",\n",
    "        (\n",
    "            \"jazz saxophone solo\",\n",
    "            \"fusion jazz saxophone solo with electronic glitch effects and syncopated beats\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Melodic Embellishment\",\n",
    "        (\n",
    "            \"flute melody\",\n",
    "            \"flute melody with cascading runs and delicate trills for added expressiveness\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Harmonic Enrichment\",\n",
    "        (\n",
    "            \"bassline\",\n",
    "            \"deep bassline with walking chromatic lines and extended harmonic sequences\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Form/Structure Expansion\",\n",
    "        (\"chorus\", \"expanded chorus with layered harmonies and intricate rhythmic variations\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Instrument Enhancement\",\n",
    "        (\n",
    "            \"synth lead\",\n",
    "            \"bright synth lead with added modulation effects and stereo widening for depth\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Mood/Tonal Enhancement\",\n",
    "        (\n",
    "            \"piano chords\",\n",
    "            \"soothing piano chords with gentle reverb and subtle tape saturation for warmth\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Genre Fusion\",\n",
    "        (\n",
    "            \"reggae rhythm\",\n",
    "            \"reggae rhythm with dubstep-inspired bass drops and electronic glitches\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Melodic Embellishment\",\n",
    "        (\n",
    "            \"violin solo\",\n",
    "            \"expressive violin solo with emotive slides and delicate pizzicato accents\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Harmonic Enrichment\",\n",
    "        (\n",
    "            \"guitar strumming\",\n",
    "            \"dynamic guitar strumming with extended chord voicings and added suspended notes\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Form/Structure Expansion\",\n",
    "        (\n",
    "            \"pre-chorus\",\n",
    "            \"extended pre-chorus with building tension and additional instrumental layers\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Instrument Enhancement\",\n",
    "        (\n",
    "            \"drum fill\",\n",
    "            \"energetic drum fill with layered percussion and added room reverb for spaciousness\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Mood/Tonal Enhancement\",\n",
    "        (\"synth pad\", \"dreamy synth pad with evolving filter sweeps and atmospheric delays\"),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "reweight_group = [\n",
    "    (\n",
    "        \"Instrument Reinforcement\",\n",
    "        \"drums\",\n",
    "        (\"dynamic drums in the chorus\", \"dynamic drums in the chorus\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Mood/Tonal Brightening\",\n",
    "        \"bright\",\n",
    "        (\"bright piano melody\", \"bright piano melody\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Genre Blend\",\n",
    "        \"pop\",\n",
    "        (\"pop rock guitar riff\", \"pop rock guitar riff\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Melodic Flourish\",\n",
    "        \"optimistic\",\n",
    "        (\"optimistic flute melody\", \"optimistic flute melody\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Harmonic Enlivening\",\n",
    "        \"major\",\n",
    "        (\"uplifting major chord progression\", \"uplifting major chord progression\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Form/Structure Expansion\",\n",
    "        \"chorus\",\n",
    "        (\"extended chorus with layered vocals\", \"extended chorus with layered vocals\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Instrument Reinforcement\",\n",
    "        \"bass\",\n",
    "        (\"thumping bassline\", \"thumping bassline\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Mood/Tonal Brightening\",\n",
    "        \"cheerful\",\n",
    "        (\"cheerful brass section\", \"cheerful brass section\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Genre Blend\",\n",
    "        \"funk\",\n",
    "        (\"funk-infused guitar riff\", \"funk-infused guitar riff\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Melodic Flourish\",\n",
    "        \"joyful\",\n",
    "        (\"joyful synth melody\", \"joyful synth melody\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Harmonic Enlivening\",\n",
    "        \"seventh\",\n",
    "        (\"vibrant seventh chord progression\", \"vibrant seventh chord progression\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Form/Structure Expansion\",\n",
    "        \"bridge\",\n",
    "        (\n",
    "            \"extended bridge section with energetic build-up\",\n",
    "            \"extended bridge section with energetic build-up\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Instrument Reinforcement\",\n",
    "        \"guitar\",\n",
    "        (\"powerful guitar solo\", \"powerful guitar solo\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Mood/Tonal Brightening\",\n",
    "        \"uplifting\",\n",
    "        (\"uplifting strings arrangement\", \"uplifting strings arrangement\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Genre Blend\",\n",
    "        \"reggae\",\n",
    "        (\"reggae-inspired drum groove\", \"reggae-inspired drum groove\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Melodic Flourish\",\n",
    "        \"hopeful\",\n",
    "        (\"hopeful vocal melody\", \"hopeful vocal melody\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Harmonic Enlivening\",\n",
    "        \"major\",\n",
    "        (\"bright major chord progression\", \"bright major chord progression\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Form/Structure Expansion\",\n",
    "        \"pre-chorus\",\n",
    "        (\n",
    "            \"extended pre-chorus with dynamic instrumentation\",\n",
    "            \"extended pre-chorus with dynamic instrumentation\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Instrument Reinforcement\",\n",
    "        \"synth\",\n",
    "        (\"lush synth pads\", \"lush synth pads\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Mood/Tonal Brightening\",\n",
    "        \"vibrant\",\n",
    "        (\"vibrant brass section\", \"vibrant brass section\"),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Iterable, Iterator\n",
    "\n",
    "\n",
    "def transform_samples(\n",
    "    edit: str,\n",
    "    samples: Iterable[tuple[str, Any, tuple[str]]],\n",
    ") -> Iterator[dict[str, Any]]:\n",
    "    for edit_category, *aditional, prompts in samples:\n",
    "        yield {\n",
    "            \"Edit\": edit.title(),\n",
    "            \"Category\": edit_category,\n",
    "            \"Source Prompt\": prompts[0],\n",
    "            \"Edited Prompt\": prompts[1],\n",
    "            \"Additional\": aditional,\n",
    "        }\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, **groups: dict[str, dict[str, Any]]) -> None:\n",
    "        self.groups = groups\n",
    "\n",
    "    def __iter__(self) -> Iterator[tuple[str, tuple[str, str], BaseController]]:\n",
    "        for edit, group in self.groups.items():\n",
    "            for sample in transform_samples(edit, group):\n",
    "                edit = sample[\"Edit\"]\n",
    "                category = sample[\"Category\"]\n",
    "                source_prompt = sample[\"Source Prompt\"]\n",
    "                edited_prompt = sample[\"Edited Prompt\"]\n",
    "                prompts = [source_prompt, edited_prompt]\n",
    "                additional = sample[\"Additional\"]\n",
    "\n",
    "                if edit == \"Replace\":\n",
    "\n",
    "                    def get_controller(prompts, additional):\n",
    "                        indices = get_replacement_indices(prompts, *additional)\n",
    "                        controller = SelfAttentionLerpControllerModifier(\n",
    "                            ReplaceWordController(indices, 1),\n",
    "                        )\n",
    "\n",
    "                        return controller\n",
    "                elif edit == \"Refine\":\n",
    "\n",
    "                    def get_controller(prompts, additional):\n",
    "                        indices = get_refine_word_indices(prompts)\n",
    "                        controller = SelfAttentionLerpControllerModifier(\n",
    "                            RefineController(indices, 1),\n",
    "                        )\n",
    "\n",
    "                        return controller\n",
    "                elif edit == \"Reweight\":\n",
    "\n",
    "                    def get_controller(prompts, additional):\n",
    "                        indices = get_reweight_word_indices(prompts, *additional)\n",
    "                        controller = SelfAttentionLerpControllerModifier(\n",
    "                            ReweightWordController(indices, 2),\n",
    "                        )\n",
    "\n",
    "                        return controller\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "\n",
    "                for seed in range(5):\n",
    "                    yield (\n",
    "                        edit,\n",
    "                        category,\n",
    "                        source_prompt,\n",
    "                        edited_prompt,\n",
    "                        seed,\n",
    "                        get_controller(prompts, additional),\n",
    "                    )\n",
    "\n",
    "    def __len__(self):\n",
    "        count = 0\n",
    "        for group in self.groups.values():\n",
    "            count += 5 * len(group)\n",
    "\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(\n",
    "    replace=replace_group,\n",
    "    refine=refine_group,\n",
    "    reweight=reweight_group,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "EVAL_DIR = RESULTS_DIR / \"Evaluation\"\n",
    "RESULTS_PATH = EVAL_DIR / \"results.pkl\"\n",
    "\n",
    "# Load existing results if available\n",
    "df = pd.DataFrame()\n",
    "if RESULTS_PATH.is_file():\n",
    "    df = pd.read_pickle(RESULTS_PATH)\n",
    "\n",
    "results = df.to_dict(orient=\"records\")\n",
    "existing_paths = set(df.get(\"Source Path\", [])).union(set(df.get(\"Edited Path\", [])))\n",
    "\n",
    "for edit, category, source_prompt, edited_prompt, seed, controller in tqdm(dataset):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    output_folder = EVAL_DIR / edit / f\"{source_prompt} - {edited_prompt}\" / f\"{seed:02d}\"\n",
    "\n",
    "    source_filepath = output_folder / f\"00 - {source_prompt}.wav\"\n",
    "    edited_filepath = output_folder / f\"01 - {edited_prompt}.wav\"\n",
    "\n",
    "    if source_filepath in existing_paths and edited_filepath in existing_paths:\n",
    "        continue\n",
    "\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        audio_values = run_and_display(\n",
    "            [source_prompt, edited_prompt],\n",
    "            output_folder,\n",
    "            controller,\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "        write(source_filepath, rate=model_proxy.sampling_rate, data=audio_values[0])\n",
    "        write(edited_filepath, rate=model_proxy.sampling_rate, data=audio_values[1])\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"{edit} '{source_prompt}' -> '{edited_prompt}' [{e}]\")\n",
    "        continue\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"Source Path\": source_filepath,\n",
    "            \"Edited Path\": edited_filepath,\n",
    "            \"Edit\": edit,\n",
    "            \"Category\": category,\n",
    "            \"Source Prompt\": source_prompt,\n",
    "            \"Edited Prompt\": edited_prompt,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    pd.DataFrame(results).to_pickle(RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
