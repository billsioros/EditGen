{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "\n",
    "if (Path(\"/\") / \"home\" / \"vsioros\" / \"data\").is_dir():\n",
    "    BASE_DIR = Path(\"/\") / \"home\" / \"vsioros\" / \"data\"\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "RESULTS_DIR = BASE_DIR / \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Any, Generator\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EditType(Enum):\n",
    "    \"\"\"An enumeration representing different types of edits.\n",
    "\n",
    "    Attributes:\n",
    "        REFINE (str): Represents a refinement edit.\n",
    "        REPLACE (str): Represents a replacement edit.\n",
    "        REWEIGHT (str): Represents a reweighting edit.\n",
    "    \"\"\"\n",
    "\n",
    "    REFINE = \"Refine\"\n",
    "    REPLACE = \"Replace\"\n",
    "    REWEIGHT = \"Reweight\"\n",
    "\n",
    "\n",
    "class ModelName(Enum):\n",
    "    \"\"\"An enumeration representing different model names.\n",
    "\n",
    "    Attributes:\n",
    "        MUSIC_GEN (str): Model name for music generation.\n",
    "        AUFFUSION (str): Model name for auffusion.\n",
    "    \"\"\"\n",
    "\n",
    "    MUSIC_GEN = \"musicgen\"\n",
    "    AUFFUSION = \"auffusion\"\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"A class for loading audio samples corresponding to a specific edit type.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        edit_type: EditType,\n",
    "        model_name: ModelName,\n",
    "        root_dir: Path = RESULTS_DIR,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the DataLoader.\n",
    "\n",
    "        Args:\n",
    "            edit_type (EditType): The type of edits to load audio samples for.\n",
    "            model_name (ModelName): The name of the model associated with the audio samples.\n",
    "            root_dir (Path, optional): The root directory containing the audio samples. Defaults to RESULTS_DIR.\n",
    "        \"\"\"\n",
    "        self.edit_type = edit_type\n",
    "        self.data_dir = root_dir / model_name.value / \"Evaluation\" / edit_type.value\n",
    "        self.count = 0\n",
    "        for prompt_dir in filter(lambda _: _.is_dir(), self.data_dir.iterdir()):\n",
    "            for _ in filter(lambda _: _.is_dir(), prompt_dir.iterdir()):\n",
    "                self.count += 1\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Get the total number of seed directories.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of seed directories.\n",
    "        \"\"\"\n",
    "        return self.count\n",
    "\n",
    "    def __iter__(self) -> Generator[tuple[str, int, dict[str, dict[str, Any]]], None, None]:\n",
    "        \"\"\"Load audio samples corresponding to the provided edit type.\n",
    "\n",
    "        Yields:\n",
    "            tuple[str, int, dict[str, Dict[str, Any]]]: A tuple containing the stem of the directory,\n",
    "            the stem of the seed directory, and a dictionary of loaded audio files along with their sampling rates.\n",
    "            The dictionary keys indicate whether the audio is the edited version (\"edited\") or the source version (\"source\").\n",
    "        \"\"\"\n",
    "        for prompt_dir in filter(lambda _: _.is_dir(), self.data_dir.iterdir()):\n",
    "            source_prompt, edited_prompt = prompt_dir.stem.split(\" - \")\n",
    "            for seed_dir in filter(lambda _: _.is_dir(), prompt_dir.iterdir()):\n",
    "                audios = {}\n",
    "                for audio_file in seed_dir.glob(\"*.wav\"):\n",
    "                    audio, sampling_rate = librosa.load(audio_file, sr=None)\n",
    "\n",
    "                    if audio_file.stem.startswith(\"00\"):\n",
    "                        audios[\"source\"] = {\n",
    "                            \"prompt\": source_prompt,\n",
    "                            \"data\": audio,\n",
    "                            \"sr\": sampling_rate,\n",
    "                        }\n",
    "                    elif audio_file.stem.startswith(\"01\"):\n",
    "                        audios[\"edited\"] = {\n",
    "                            \"prompt\": edited_prompt,\n",
    "                            \"data\": audio,\n",
    "                            \"sr\": sampling_rate,\n",
    "                        }\n",
    "\n",
    "                yield int(seed_dir.stem), audios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melody Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def extract_pitch_classes(audio: np.ndarray, sr: int, hop_length: int = 512) -> np.ndarray:\n",
    "    \"\"\"Extract pitch classes from an audio signal.\n",
    "\n",
    "    Args:\n",
    "        audio (np.ndarray): Input audio signal.\n",
    "        sr (int): Sampling rate of the audio signal.\n",
    "        hop_length (int): Hop length for computing the pitch.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of pitch classes.\n",
    "    \"\"\"\n",
    "    # Extract pitch using librosa's piptrack function\n",
    "    _, magnitudes = librosa.core.piptrack(y=audio, sr=sr, hop_length=hop_length)\n",
    "\n",
    "    # Get the pitch with the maximum magnitude for each frame\n",
    "    return np.argmax(magnitudes, axis=0)\n",
    "\n",
    "\n",
    "def melody_accuracy(source_audio: np.ndarray, generated_audio: np.ndarray, sr: int) -> float:\n",
    "    \"\"\"Calculate the accuracy of generated melody compared to the input melody.\n",
    "\n",
    "    Args:\n",
    "        source_audio (np.ndarray): Input audio.\n",
    "        generated_audio (np.ndarray): Generated audio.\n",
    "        sr: (int): The sampling rate of the provided audio files.\n",
    "\n",
    "    Returns:\n",
    "        float: Melody accuracy.\n",
    "    \"\"\"\n",
    "    # Extract pitch classes from both melodies\n",
    "    input_pitch_classes = extract_pitch_classes(source_audio, sr)\n",
    "    generated_pitch_classes = extract_pitch_classes(generated_audio, sr)\n",
    "\n",
    "    # Calculate melody accuracy\n",
    "    return accuracy_score(input_pitch_classes, generated_pitch_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "def extract_dynamics(audio: np.ndarray, sr: int) -> np.ndarray:\n",
    "    \"\"\"Extract dynamics from an audio file.\n",
    "\n",
    "    Args:\n",
    "        audio (np.ndarray): The audio file.\n",
    "        sr: (int): The sampling rate of the provided audio file.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array containing the dynamics of the audio.\n",
    "    \"\"\"\n",
    "    onset_env = librosa.onset.onset_strength(y=audio, sr=sr)\n",
    "    tempo, _ = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)\n",
    "    return onset_env / tempo\n",
    "\n",
    "\n",
    "def dynamics_correlation(\n",
    "    source_audio: np.ndarray,\n",
    "    edited_audio: np.ndarray,\n",
    "    sr: int,\n",
    ") -> float:\n",
    "    \"\"\"Compute macro dynamics correlation between two audio files.\n",
    "\n",
    "    Args:\n",
    "        source_audio (np.ndarray): The source audio file.\n",
    "        edited_audio (np.ndarray): The edited audio file.\n",
    "        sr (int): The sampling rate of the provided audio files.\n",
    "\n",
    "    Returns:\n",
    "        float: The macro dynamics correlation.\n",
    "    \"\"\"\n",
    "    source_dynamics = extract_dynamics(source_audio, sr)\n",
    "    edited_dynamics = extract_dynamics(edited_audio, sr)\n",
    "    macro_correlation, _ = pearsonr(source_dynamics, edited_dynamics)\n",
    "\n",
    "    return macro_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: MICRO CORRELATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rythm F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "# This is done to circumvent deprecation issues\n",
    "np.float = float\n",
    "np.int = int\n",
    "\n",
    "from madmom.features.beats import DBNBeatTrackingProcessor, RNNBeatProcessor\n",
    "\n",
    "\n",
    "def extract_beat_timestamps(activation: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Extract beat timestamps from the given activation.\n",
    "\n",
    "    Args:\n",
    "        activation (np.ndarray): Activation data for beat tracking.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array containing beat timestamps.\n",
    "    \"\"\"\n",
    "    return DBNBeatTrackingProcessor(fps=100)(activation)\n",
    "\n",
    "\n",
    "def check_alignment(\n",
    "    timestamps_1: List[float],\n",
    "    timestamps_2: List[float],\n",
    "    threshold: float = 0.07,\n",
    ") -> List[Tuple[float, float]]:\n",
    "    \"\"\"Check alignment between two sets of timestamps.\n",
    "\n",
    "    Args:\n",
    "        timestamps_1 (List[float]): Timestamps from the first audio file.\n",
    "        timestamps_2 (List[float]): Timestamps from the second audio file.\n",
    "        threshold (float, optional): Maximum allowed difference for alignment. Defaults to 0.07.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[float, float]]: List of aligned timestamps pairs.\n",
    "    \"\"\"\n",
    "    aligned = []\n",
    "    for ts1 in timestamps_1:\n",
    "        for ts2 in timestamps_2:\n",
    "            if abs(ts1 - ts2) < threshold:\n",
    "                aligned.append((ts1, ts2))\n",
    "                break\n",
    "    return aligned\n",
    "\n",
    "\n",
    "def calculate_f1_score(\n",
    "    aligned_timestamps: List[Tuple[float, float]],\n",
    "    timestamps_1: List[float],\n",
    "    timestamps_2: List[float],\n",
    ") -> float:\n",
    "    \"\"\"Calculate the F1 score based on alignment of timestamps.\n",
    "\n",
    "    Args:\n",
    "        aligned_timestamps (List[Tuple[float, float]]): List of aligned timestamp pairs.\n",
    "        timestamps_1 (List[float]): Timestamps from the first audio file.\n",
    "        timestamps_2 (List[float]): Timestamps from the second audio file.\n",
    "\n",
    "    Returns:\n",
    "        float: F1 score.\n",
    "    \"\"\"\n",
    "    precision = len(aligned_timestamps) / len(timestamps_1)\n",
    "    recall = len(aligned_timestamps) / len(timestamps_2)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def rhythm_f1(source_audio: np.ndarray, generated_audio: np.ndarray) -> float:\n",
    "    \"\"\"Calculate Rhythm F1 score between two audio samples.\n",
    "\n",
    "    Read more [here](https://madmom.readthedocs.io/en/v0.15/modules/features/beats.html#madmom.features.beats.DBNBeatTrackingProcessor).\n",
    "\n",
    "    Args:\n",
    "        source_audio (np.ndarray): Array containing the first audio sample.\n",
    "        generated_audio (np.ndarray): Array containing the second audio sample.\n",
    "\n",
    "    Returns:\n",
    "        float: Rhythm F1 score.\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate frame-wise beat probabilities\n",
    "    beat_processor_1 = RNNBeatProcessor()\n",
    "    beat_processor_2 = RNNBeatProcessor()\n",
    "\n",
    "    # Step 2: Extract beat timestamps\n",
    "    timestamps_1 = extract_beat_timestamps(beat_processor_1(source_audio))\n",
    "    timestamps_2 = extract_beat_timestamps(beat_processor_2(generated_audio))\n",
    "\n",
    "    # Step 3: Check alignment\n",
    "    aligned_timestamps = check_alignment(timestamps_1, timestamps_2)\n",
    "\n",
    "    # Step 4: Calculate F1 score\n",
    "    return calculate_f1_score(aligned_timestamps, timestamps_1, timestamps_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLAP Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, ClapModel\n",
    "\n",
    "clap_model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "clap_processor = AutoProcessor.from_pretrained(\"laion/clap-htsat-unfused\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def clap(\n",
    "    prompt: str,\n",
    "    audios: np.ndarray,\n",
    "    orig_sr: int,\n",
    "    sr: int = 48000,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Computes cosine similarity between a prompt and audio features.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt.\n",
    "        audios (np.ndarray): Array containing audio samples.\n",
    "        orig_sr (int): Original sampling rate of audio.\n",
    "        sr (int, optional): Target sampling rate. Defaults to 48000.\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float]: Cosine similarity between audios.\n",
    "    \"\"\"\n",
    "    # Resample audios\n",
    "    if orig_sr is not None and orig_sr != sr:\n",
    "        audios = np.stack(\n",
    "            [librosa.resample(audio, orig_sr=orig_sr, target_sr=sr) for audio in audios],\n",
    "        )\n",
    "\n",
    "    inputs = clap_processor(\n",
    "        text=prompt,\n",
    "        audios=audios,\n",
    "        return_tensors=\"pt\",\n",
    "        sampling_rate=sr,\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    # Process prompt and audios\n",
    "    prompt_features = clap_model.get_text_features(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "    )\n",
    "    audio_features = clap_model.get_audio_features(\n",
    "        input_features=inputs[\"input_features\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "    )\n",
    "\n",
    "    # Calculate cosine similarity between audios\n",
    "    audio_audio_similarity = F.cosine_similarity(audio_features[0], audio_features[1], dim=0)\n",
    "\n",
    "    # Calculate cosine similarity between prompt at index 1 and audio at index 1\n",
    "    text_audio_similarity = F.cosine_similarity(prompt_features[0], audio_features[1], dim=0)\n",
    "\n",
    "    return audio_audio_similarity.item(), text_audio_similarity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def collect_metrics(\n",
    "    prompt: str,\n",
    "    source_audio: np.ndarray,\n",
    "    generated_audio: np.ndarray,\n",
    "    sr: int,\n",
    ") -> tuple[float, float, float, float]:\n",
    "    \"\"\"Collect metrics for the provided source and generated audio.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt associated with the audio samples.\n",
    "        source_audio (np.ndarray): The source audio samples.\n",
    "        generated_audio (np.ndarray): The generated audio samples.\n",
    "        sr (int): The sampling rate of the audio samples.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float, float]: A tuple containing the computed metrics.\n",
    "            - Melody accuracy score\n",
    "            - Dynamics correlation score\n",
    "            - Rhythm F1 score\n",
    "            - Clap score\n",
    "    \"\"\"\n",
    "    melody_accuracy_score = melody_accuracy(source_audio, generated_audio, sr)\n",
    "    dynamics_correlation_score = dynamics_correlation(source_audio, generated_audio, sr)\n",
    "    rhythm_f1_score = rhythm_f1(source_audio, generated_audio)\n",
    "    clap_score = clap(prompt, [source_audio, generated_audio], sr)\n",
    "\n",
    "    return melody_accuracy_score, dynamics_correlation_score, rhythm_f1_score, clap_score\n",
    "\n",
    "\n",
    "def construct_dataframe(data_loader: DataLoader) -> pd.DataFrame:\n",
    "    \"\"\"Construct a DataFrame containing metrics for audio samples.\n",
    "\n",
    "    This function constructs a DataFrame containing metrics for audio samples loaded\n",
    "    using the provided DataLoader.\n",
    "\n",
    "    Args:\n",
    "        data_loader (DataLoader): An instance of DataLoader that loads audio samples.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing metrics for audio samples.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for seed, samples in tqdm(data_loader, position=2, leave=False, desc=\"Evaluating dataset\"):\n",
    "        source_prompt = samples.get(\"source\", {}).get(\"prompt\")\n",
    "        edited_prompt = samples.get(\"edited\", {}).get(\"prompt\")\n",
    "        source_audio = samples.get(\"source\", {}).get(\"data\")\n",
    "        generated_audio = samples.get(\"edited\", {}).get(\"data\")\n",
    "        sr = samples.get(\"source\", {}).get(\"sr\")  # Assuming source and edited have the same sr\n",
    "\n",
    "        if source_audio is not None and generated_audio is not None:\n",
    "            # Calculate metrics\n",
    "            melody_accuracy_score, dynamics_correlation_score, rhythm_f1_score, clap_score = (\n",
    "                collect_metrics(\n",
    "                    samples.get(\"source\", {}).get(\"prompt\"),\n",
    "                    source_audio,\n",
    "                    generated_audio,\n",
    "                    sr,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Append results to DataFrame\n",
    "            results.append(\n",
    "                {\n",
    "                    \"Source Prompt\": source_prompt,\n",
    "                    \"Edited Prompt\": edited_prompt,\n",
    "                    \"Seed\": seed,\n",
    "                    \"Melody Accuracy\": melody_accuracy_score,\n",
    "                    \"Dynamics Correlation\": dynamics_correlation_score,\n",
    "                    \"Rhythm F1\": rhythm_f1_score,\n",
    "                    \"A2A Similarity\": clap_score[0],\n",
    "                    \"T2A Similarity\": clap_score[1],\n",
    "                },\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(results).sort_values(by=[\"Source Prompt\", \"Seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in tqdm(ModelName):\n",
    "    for edit_type in tqdm(EditType, position=1, leave=False):\n",
    "        df = construct_dataframe(DataLoader(edit_type, model_name))\n",
    "        DATAFRAME_PATH = (\n",
    "            RESULTS_DIR / model_name.value / \"Evaluation\" / edit_type.value / \"metrics.pkl\"\n",
    "        )\n",
    "        df.to_pickle(DATAFRAME_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for model_name in ModelName:\n",
    "    for edit_type in EditType:\n",
    "        DATAFRAME_PATH = (\n",
    "            RESULTS_DIR / model_name.value / \"Evaluation\" / edit_type.value / \"metrics.pkl\"\n",
    "        )\n",
    "        df = pd.read_pickle(DATAFRAME_PATH)\n",
    "        df['Model'] = model_name.value\n",
    "        df['Edit'] = edit_type.value\n",
    "        dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    \"Melody Accuracy\",\n",
    "    \"Dynamics Correlation\",\n",
    "    \"Rhythm F1\",\n",
    "    \"A2A Similarity\",\n",
    "    \"T2A Similarity\",\n",
    "]\n",
    "\n",
    "df.groupby(by=[\"Model\", \"Edit\"])[metrics].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
