{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "70300319-d206-43ce-b3bf-3da6b079f20f"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "e70e6dbb-3211-4ef9-93f6-efaba764ac77"
   },
   "source": [
    "## Prepare the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "MODEL_NAME = \"facebook/musicgen-small\"\n",
    "\n",
    "if (Path(\"/\") / \"home\" / \"vsioros\" / \"data\").is_dir():\n",
    "    BASE_DIR = Path(\"/\") / \"home\" / \"vsioros\" / \"data\"\n",
    "    MODEL_NAME = \"facebook/musicgen-large\"\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "77ee39cc-654b-4f0e-b601-013e484c16f0"
   },
   "source": [
    "## Load the Model\n",
    "\n",
    "The pre-trained MusicGen small, medium and large checkpoints can be loaded from the [pre-trained weights](https://huggingface.co/models?search=facebook/musicgen-) on the Hugging Face Hub. Change the repo id with the checkpoint size you wish to load. We'll default to the small checkpoint, which is the fastest of the three but has the lowest audio quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "b0d87424-9f38-4658-ba47-2a465d52ad77"
   },
   "outputs": [],
   "source": [
    "from transformers import MusicgenForConditionalGeneration\n",
    "from transformers import AutoProcessor\n",
    "import torch\n",
    "from typing import Any\n",
    "from numpy.typing import NDArray\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ModelProxy(object):\n",
    "    def __init__(self, model_name: str, guidance_scale: float = 3.0):\n",
    "        self.model_name = model_name\n",
    "        self.guidance_scale = guidance_scale\n",
    "\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.processor = AutoProcessor.from_pretrained(model_name)\n",
    "        self.model = MusicgenForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "\n",
    "    def generate(self, inputs: dict[str, Any], max_new_tokens: int = 512) -> NDArray[np.float_]:\n",
    "        return (\n",
    "            self.model.generate(\n",
    "                **inputs.to(self.device),\n",
    "                do_sample=True,\n",
    "                guidance_scale=self.guidance_scale,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "            )\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "            .squeeze()\n",
    "        )\n",
    "\n",
    "    def encode(self, prompts: list[str]) -> dict[str, Any]:\n",
    "        return self.processor(text=prompts, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    def decode(self, encoded_token: NDArray[np.float_]) -> str:\n",
    "        return self.processor.decode(encoded_token)\n",
    "\n",
    "    @property\n",
    "    def sampling_rate(self) -> float:\n",
    "        return self.model.config.audio_encoder.sampling_rate\n",
    "\n",
    "    @property\n",
    "    def frame_rate(self) -> float:\n",
    "        return self.model.config.audio_encoder.frame_rate\n",
    "\n",
    "    @property\n",
    "    def decoder_layers(self) -> list[torch.nn.Module]:\n",
    "        return self.model.decoder.model.decoder.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_proxy = ModelProxy(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Prompt-to-Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers.utils import (\n",
    "    logging,\n",
    ")\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "def register_attention_control(model_proxy, controller):\n",
    "    def ca_forward(self, attention_type):\n",
    "        def forward(\n",
    "            hidden_states: torch.Tensor,\n",
    "            key_value_states: Optional[torch.Tensor] = None,\n",
    "            past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            layer_head_mask: Optional[torch.Tensor] = None,\n",
    "            output_attentions: bool = False,\n",
    "        ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "            \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "            # if key_value_states are provided this layer is used as a cross-attention layer\n",
    "            # for the decoder\n",
    "            is_cross_attention = key_value_states is not None\n",
    "\n",
    "            bsz, tgt_len, _ = hidden_states.size()\n",
    "\n",
    "            # get query proj\n",
    "            query_states = self.q_proj(hidden_states) * self.scaling\n",
    "            # get key, value proj\n",
    "            # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n",
    "            # is checking that the `sequence_length` of the `past_key_value` is the same as\n",
    "            # the provided `key_value_states` to support prefix tuning\n",
    "            if (\n",
    "                is_cross_attention\n",
    "                and past_key_value is not None\n",
    "                and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
    "            ):\n",
    "                # reuse k,v, cross_attentions\n",
    "                key_states = past_key_value[0]\n",
    "                value_states = past_key_value[1]\n",
    "            elif is_cross_attention:\n",
    "                # cross_attentions\n",
    "                key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
    "                value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
    "            elif past_key_value is not None:\n",
    "                # reuse k, v, self_attention\n",
    "                key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "                value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "                key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
    "                value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
    "            else:\n",
    "                # self_attention\n",
    "                key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "                value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "            if self.is_decoder:\n",
    "                # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "                # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "                # key/value_states (first \"if\" case)\n",
    "                # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "                # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "                # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "                # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "                past_key_value = (key_states, value_states)\n",
    "\n",
    "            proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "            query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "            key_states = key_states.reshape(*proj_shape)\n",
    "            value_states = value_states.reshape(*proj_shape)\n",
    "\n",
    "            src_len = key_states.size(1)\n",
    "            attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
    "\n",
    "            if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
    "                    f\" {attn_weights.size()}\",\n",
    "                )\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                    raise ValueError(\n",
    "                        f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\",\n",
    "                    )\n",
    "                attn_weights = (\n",
    "                    attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
    "                )\n",
    "                attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "            attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "            attn_weights = controller(attn_weights, is_cross_attention, attention_type)\n",
    "\n",
    "            if layer_head_mask is not None:\n",
    "                if layer_head_mask.size() != (self.num_heads,):\n",
    "                    raise ValueError(\n",
    "                        f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n",
    "                        f\" {layer_head_mask.size()}\",\n",
    "                    )\n",
    "                attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(\n",
    "                    bsz, self.num_heads, tgt_len, src_len\n",
    "                )\n",
    "                attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "            if output_attentions:\n",
    "                # this operation is a bit awkward, but it's required to\n",
    "                # make sure that attn_weights keeps its gradient.\n",
    "                # In order to do so, attn_weights have to be reshaped\n",
    "                # twice and have to be reused in the following\n",
    "                attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "                attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "            else:\n",
    "                attn_weights_reshaped = None\n",
    "\n",
    "            attn_probs = nn.functional.dropout(\n",
    "                attn_weights, p=self.dropout, training=self.training\n",
    "            )\n",
    "\n",
    "            attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "            if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "                raise ValueError(\n",
    "                    f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
    "                    f\" {attn_output.size()}\",\n",
    "                )\n",
    "\n",
    "            attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "            attn_output = attn_output.transpose(1, 2)\n",
    "\n",
    "            # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n",
    "            # partitioned across GPUs when using tensor-parallelism.\n",
    "            attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
    "\n",
    "            attn_output = self.out_proj(attn_output)\n",
    "\n",
    "            return attn_output, attn_weights_reshaped, past_key_value\n",
    "\n",
    "        return forward\n",
    "\n",
    "    def register_recr(net_, count, attention_type):\n",
    "        if net_.__class__.__name__ == \"MusicgenAttention\":\n",
    "            net_.forward = ca_forward(net_, attention_type)\n",
    "            return count + 1\n",
    "\n",
    "        return count\n",
    "\n",
    "    cross_att_count = 0\n",
    "    sub_nets = model_proxy.decoder_layers.named_children()\n",
    "    for net in sub_nets:\n",
    "        if net[1].__class__.__name__ != \"MusicgenDecoderLayer\":\n",
    "            continue\n",
    "\n",
    "        for subnet in net[1].named_children():\n",
    "            attention_type = None\n",
    "            if subnet[0] == \"encoder_attn\":\n",
    "                attention_type = \"cross\"\n",
    "            elif subnet[0] == \"self_attn\":\n",
    "                attention_type = \"self\"\n",
    "\n",
    "            if attention_type is not None:\n",
    "                cross_att_count += register_recr(subnet[1], 0, attention_type)\n",
    "\n",
    "    controller.num_att_layers = cross_att_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class BaseController(ABC):\n",
    "    def reset(self):\n",
    "        self.num_att_layers = 0\n",
    "        self.batch_size = -1\n",
    "        self.max_new_tokens = -1\n",
    "\n",
    "        self.cur_att_layer = 0\n",
    "        self.cur_step = 0\n",
    "\n",
    "    def __call__(self, attn_weights, is_cross, attention_type) -> None:\n",
    "        self.cur_att_layer = self.cur_att_layer + 1\n",
    "        if self.cur_att_layer == self.num_att_layers:\n",
    "            self.cur_att_layer = 0\n",
    "            self.cur_step = self.cur_step + 1\n",
    "\n",
    "        # Exclude unconditional inputs\n",
    "        h1 = attn_weights.shape[0] // 2\n",
    "        attn = attn_weights[:h1]\n",
    "\n",
    "        # Reshape according to batch size\n",
    "        h2 = attn.shape[0] // (self.batch_size)\n",
    "\n",
    "        attn = attn.reshape(self.batch_size, h2, *attn.shape[1:])\n",
    "\n",
    "        if is_cross:\n",
    "            attn = self.replace_cross_attention(attn, is_cross, attention_type)\n",
    "            # attn /= torch.sum(attn, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            attn = self.replace_self_attention(attn)\n",
    "\n",
    "        attn = attn.reshape(self.batch_size * h2, *attn.shape[2:])\n",
    "\n",
    "        attn_weights[:h1] = attn\n",
    "\n",
    "        return attn_weights\n",
    "\n",
    "    @abstractmethod\n",
    "    def replace_self_attention(self, attn):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class EmptyController(BaseController):\n",
    "    def replace_self_attention(self, attn):\n",
    "        return attn\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type):\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class AttentionStore(BaseController):\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "\n",
    "        self.features = defaultdict(lambda: [])\n",
    "\n",
    "    def get_self_attention(self):\n",
    "        tensors = self.features[\"self\"]\n",
    "        tensors = [tensor.mean(dim=-1) for tensor in tensors]\n",
    "        tensors = torch.stack(tensors)\n",
    "        tensors = tensors.view(self.max_new_tokens, -1, *tensors.shape[1:])\n",
    "\n",
    "        return tensors\n",
    "\n",
    "    def get_cross_attention(self):\n",
    "        tensors = self.features[\"cross\"]\n",
    "        tensors = torch.stack(tensors)\n",
    "        tensors = tensors.view(self.max_new_tokens, -1, *tensors[0].shape)\n",
    "\n",
    "        return tensors\n",
    "\n",
    "    def get_self_attention_importance(self):\n",
    "        aggregate_cross_attention = self.get_self_attention()\n",
    "        aggregate_cross_attention = aggregate_cross_attention[:, :, 1:, :, :]\n",
    "        aggregate_cross_attention = aggregate_cross_attention.mean(dim=(0, 3, 4))\n",
    "\n",
    "        # Min-Max scaling to normalize values between 0 and 1 for each column (sample)\n",
    "        min_values = aggregate_cross_attention.min(dim=0).values\n",
    "        max_values = aggregate_cross_attention.max(dim=0).values\n",
    "\n",
    "        normalized_scores = (aggregate_cross_attention - min_values) / (max_values - min_values)\n",
    "\n",
    "        # Get indices that would sort the layers based on their mean scores\n",
    "        sorted_indices = torch.argsort(normalized_scores, descending=True, dim=0)\n",
    "        sorted_indices = sorted_indices.view(sorted_indices.shape[1], -1)\n",
    "\n",
    "        # self-attention layers are called first and thusly hold indices 1, 3, 5 etc.\n",
    "        sorted_indices = 2 * sorted_indices + 1\n",
    "\n",
    "        return sorted_indices\n",
    "\n",
    "    def get_cross_attention_importance(self, word_piece_index):\n",
    "        aggregate_cross_attention = self.get_cross_attention()\n",
    "        aggregate_cross_attention = aggregate_cross_attention[:, :, 1:, :, :, word_piece_index]\n",
    "        aggregate_cross_attention = aggregate_cross_attention.mean(dim=(0, 3, 4))\n",
    "\n",
    "        # Min-Max scaling to normalize values between 0 and 1 for each column (sample)\n",
    "        min_values = aggregate_cross_attention.min(dim=0).values\n",
    "        max_values = aggregate_cross_attention.max(dim=0).values\n",
    "\n",
    "        normalized_scores = (aggregate_cross_attention - min_values) / (max_values - min_values)\n",
    "\n",
    "        # Get indices that would sort the layers based on their mean scores\n",
    "        sorted_indices = torch.argsort(normalized_scores, descending=True, dim=0)\n",
    "        sorted_indices = sorted_indices.view(sorted_indices.shape[1], -1)\n",
    "\n",
    "        # cross-attention layers are called second and thusly hold indices 2, 4, 6 etc.\n",
    "        sorted_indices = 2 * (sorted_indices + 1)\n",
    "\n",
    "        return sorted_indices\n",
    "\n",
    "    def get_aggregate_cross_attention(self):\n",
    "        return torch.mean(torch.stack(self.features[\"cross\"]), axis=0)\n",
    "\n",
    "    def replace_self_attention(self, attn) -> None:\n",
    "        self.features[\"self\"].append(attn)\n",
    "\n",
    "        return attn\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type) -> None:\n",
    "        self.features[\"cross\"].append(attn_weights)\n",
    "\n",
    "        return attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEditController(BaseController):\n",
    "    def replace_self_attention(self, attn):\n",
    "        attn_base, att_replace = attn[0], attn[1:]\n",
    "\n",
    "        return attn_base.unsqueeze(0).expand(att_replace.shape[0] + 1, *attn_base.shape)\n",
    "\n",
    "\n",
    "class RandomController(BaseEditController):\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type):\n",
    "        attn_weights[1:] = torch.randn_like(attn_weights[0])\n",
    "\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class IgnoreWordController(BaseEditController):\n",
    "    def __init__(self, indices: list[int]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.indices = indices\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type):\n",
    "        attn_weights[1:, :, :, self.indices] = 0\n",
    "\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class ReplaceWordController(BaseEditController):\n",
    "    def __init__(self, indices: list[list[int]], blend: float = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.source_indices, self.target_indices = indices\n",
    "        self.blend = blend\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type):\n",
    "        source_attn = attn_weights[0, :, :, self.source_indices]\n",
    "        averaged_attn = source_attn.mean(dim=-1, keepdims=True)\n",
    "\n",
    "        # Repeat averaged attention values to match dimensions of the target attention\n",
    "        averaged_attn_repeated = averaged_attn.expand(-1, -1, len(self.target_indices))\n",
    "\n",
    "        attn_weights[1:, :, :, self.target_indices] = (1 - self.blend) * attn_weights[\n",
    "            1:, :, :, self.target_indices\n",
    "        ] + self.blend * averaged_attn_repeated\n",
    "\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class RefineController(BaseEditController):\n",
    "    def __init__(self, indices: list[list[int]], blend: float = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.source_indices, self.target_indices = indices\n",
    "        self.blend = blend\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type):\n",
    "        attn_weights[1:, :, :, self.target_indices] = (1 - self.blend) * attn_weights[\n",
    "            1:, :, :, self.target_indices\n",
    "        ] + self.blend * attn_weights[0, :, :, self.source_indices]\n",
    "\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class ReweightWordController(BaseEditController):\n",
    "    def __init__(self, indices: list[int], weight: float = 5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.indices = indices\n",
    "        self.weight = weight\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type):\n",
    "        attn_weights[1:] = attn_weights[0]\n",
    "\n",
    "        non_target_indices = [i for i in range(attn_weights.shape[-1]) if i not in self.indices]\n",
    "        attn_weights[1:, :, :, non_target_indices] /= self.weight\n",
    "\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class ReplaceController(BaseEditController):\n",
    "    def __init__(self, blend: float = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blend = blend\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type):\n",
    "        attn_weights[1:] = (1 - self.blend) * attn_weights[1:] + self.blend * attn_weights[0]\n",
    "\n",
    "        return attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class ControllerModifier(BaseController):\n",
    "    def __init__(self, controller: BaseController) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.controller = controller\n",
    "\n",
    "    def __getattr__(self, name: str):\n",
    "        if name == \"controller\":\n",
    "            return super().__getattr__(name)\n",
    "\n",
    "        return getattr(self.controller, name)\n",
    "\n",
    "    def __setattr__(self, name: str, value: Any):\n",
    "        if name == \"controller\":\n",
    "            return super().__setattr__(name, value)\n",
    "\n",
    "        return setattr(self.controller, name, value)\n",
    "\n",
    "\n",
    "class OffsetControllerModifier(ControllerModifier):\n",
    "    def __init__(self, controller: BaseController, offset: float = 0.0) -> None:\n",
    "        super().__init__(controller)\n",
    "\n",
    "        assert 0.0 <= offset <= 1.0\n",
    "\n",
    "        self.offset = offset\n",
    "\n",
    "    def replace_self_attention(self, attn):\n",
    "        if self.cur_step < round(self.offset * self.max_new_tokens):\n",
    "            return attn\n",
    "\n",
    "        return self.controller.replace_self_attention(attn)\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type):\n",
    "        if self.cur_step < round(self.offset * self.max_new_tokens):\n",
    "            return attn_weights\n",
    "\n",
    "        return self.controller.replace_cross_attention(attn_weights, is_cross, attention_type)\n",
    "\n",
    "\n",
    "class AttentionHeadControllerModifier(ControllerModifier):\n",
    "    def __init__(self, controller: BaseController, attention_head_indices: list[int]) -> None:\n",
    "        super().__init__(controller)\n",
    "\n",
    "        self.attention_head_indices = attention_head_indices\n",
    "\n",
    "    def replace_self_attention(self, attn):\n",
    "        return self.controller.replace_self_attention(attn)\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type):\n",
    "        attn_weights_slice = attn_weights[:, self.attention_head_indices, :, :]\n",
    "        attn_weights_slice = self.controller.replace_cross_attention(\n",
    "            attn_weights_slice, is_cross, attention_type\n",
    "        )\n",
    "\n",
    "        attn_weights[:, self.attention_head_indices, :, :] = attn_weights_slice\n",
    "\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class SelfAttentionLerpControllerModifier(ControllerModifier):\n",
    "    def __init__(self, controller: BaseController) -> None:\n",
    "        super().__init__(controller)\n",
    "\n",
    "    def replace_self_attention(self, attn):\n",
    "        blend = self.cur_att_layer / self.num_att_layers\n",
    "\n",
    "        attn[1:] = (1 - blend) * attn[1:] + blend * attn[0]\n",
    "\n",
    "        return attn\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type) -> None:\n",
    "        return self.controller.replace_cross_attention(attn_weights, is_cross, attention_type)\n",
    "\n",
    "\n",
    "class SelfAttentionCutoffControllerModifier(ControllerModifier):\n",
    "    def __init__(self, controller: BaseController, threshold: float = 0.75) -> None:\n",
    "        super().__init__(controller)\n",
    "\n",
    "        assert 0.0 <= threshold <= 1.0\n",
    "\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def replace_self_attention(self, attn):\n",
    "        if self.cur_att_layer <= np.floor(self.threshold * self.num_att_layers):\n",
    "            return self.controller.replace_self_attention(attn)\n",
    "\n",
    "        return attn\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type) -> None:\n",
    "        return self.controller.replace_cross_attention(attn_weights, is_cross, attention_type)\n",
    "\n",
    "\n",
    "class AttentionLerpControllerModifier(ControllerModifier):\n",
    "    def __init__(self, controller: BaseController) -> None:\n",
    "        super().__init__(controller)\n",
    "\n",
    "    def replace_self_attention(self, attn):\n",
    "        blend = self.cur_att_layer / self.num_att_layers\n",
    "\n",
    "        attn = blend * attn + (1 - blend) * self.controller.replace_self_attention(attn)\n",
    "\n",
    "        return attn\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type) -> None:\n",
    "        blend = self.cur_att_layer / self.num_att_layers\n",
    "\n",
    "        attn_weights = blend * attn_weights + (1 - blend) * self.controller.replace_cross_attention(attn_weights.clone(), is_cross, attention_type)\n",
    "\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class AttentionCutoffControllerModifier(ControllerModifier):\n",
    "    def __init__(self, controller: BaseController, threshold: float = 0.75) -> None:\n",
    "        super().__init__(controller)\n",
    "\n",
    "        assert 0.0 <= threshold <= 1.0\n",
    "\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def replace_self_attention(self, attn):\n",
    "        if self.cur_att_layer <= np.floor(self.threshold * self.num_att_layers):\n",
    "            return self.controller.replace_self_attention(attn)\n",
    "\n",
    "        return attn\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type) -> None:\n",
    "        if self.cur_att_layer <= np.floor(self.threshold * self.num_att_layers):\n",
    "            return self.controller.replace_cross_attention(attn_weights, is_cross, attention_type)\n",
    "\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class DecoderLayerControllerModifier(ControllerModifier):\n",
    "    def __init__(self, controller: BaseController, decoder_layer_indices: set[int]) -> None:\n",
    "        super().__init__(controller)\n",
    "\n",
    "        self.decoder_layer_indices = decoder_layer_indices\n",
    "\n",
    "    def replace_self_attention(self, attn):\n",
    "        if self.cur_att_layer in self.decoder_layer_indices:\n",
    "            return self.controller.replace_self_attention(attn)\n",
    "\n",
    "        return attn\n",
    "\n",
    "    def replace_cross_attention(self, attn_weights, is_cross, attention_type):\n",
    "        if self.cur_att_layer in self.decoder_layer_indices:\n",
    "            return self.controller.replace_cross_attention(attn_weights, is_cross, attention_type)\n",
    "\n",
    "        return attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from scipy.io.wavfile import write\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "CONTROLLER_REG = re.compile(r\"Controller.*\")\n",
    "SNAKE_CASE_REG = re.compile(r\"(?<!^)(?=[A-Z])\")\n",
    "\n",
    "RESULTS_DIR = BASE_DIR / \"results\" / \"musicgen\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def _get_controller_subpath(controller: BaseController) -> Path:\n",
    "    controller_name = CONTROLLER_REG.sub(\"\", controller.__class__.__name__)\n",
    "    controller_name = SNAKE_CASE_REG.sub(\"_\", controller_name).lower()\n",
    "\n",
    "    return Path(controller_name)\n",
    "\n",
    "\n",
    "def get_controller_subpath(controller: BaseController) -> Path:\n",
    "    if isinstance(controller, DecoderLayerControllerModifier):\n",
    "        return (\n",
    "            _get_controller_subpath(controller)\n",
    "            / f\"{''.join(map(lambda x: f'{x:02d}', controller.decoder_layer_indices))}\"\n",
    "            / get_controller_subpath(controller.controller)\n",
    "        )\n",
    "    elif isinstance(controller, ReplaceController):\n",
    "        return _get_controller_subpath(controller) / f\"{controller.blend:.2f}\"\n",
    "    elif isinstance(controller, ReplaceWordController):\n",
    "        return (\n",
    "            _get_controller_subpath(controller)\n",
    "            / f\"{controller.index:02d}\"\n",
    "            / f\"{controller.blend:.2f}\"\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(controller.__class__.__name__)\n",
    "\n",
    "\n",
    "def construct_output_folder_path(controller: BaseController) -> Path:\n",
    "    return RESULTS_DIR / get_controller_subpath(controller) / datetime.now().strftime(\"%Y_%m_%d\")\n",
    "\n",
    "\n",
    "def get_tokens(prompts: list[str]):\n",
    "    inputs = model_proxy.encode(prompts)\n",
    "\n",
    "    tokens = []\n",
    "    for index in range(len(prompts)):\n",
    "        tokens.append([model_proxy.decode(item) for item in inputs[\"input_ids\"][index]])\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_replacement_indices(prompts: list[str], word_a: str, word_b: str) -> list[list[int]]:\n",
    "    token_groups = get_tokens(prompts)\n",
    "\n",
    "    if len(prompts[0].split()) != len(prompts[1].split()):\n",
    "        raise NotImplementedError(f\"Different prompt lengths ({prompts})\")\n",
    "\n",
    "    indices = []\n",
    "    for word, tokens in zip([word_a, word_b], token_groups):\n",
    "        prompt_indices, substring = [], []\n",
    "        for i in range(len(tokens)):\n",
    "            if word.startswith(\"\".join([*substring, tokens[i]])):\n",
    "                substring.append(tokens[i])\n",
    "                prompt_indices.append(i)\n",
    "        indices.append(prompt_indices)\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "def get_reweight_word_indices(prompts: list[str], word: str) -> tuple[list[str], list[int]]:\n",
    "    return get_replacement_indices(prompts, word, word)[0]\n",
    "\n",
    "\n",
    "def get_refine_word_indices(prompts: list[str]) -> tuple[list[str], list[int]]:\n",
    "    token_groups = get_tokens(prompts)\n",
    "\n",
    "    indices = []\n",
    "    for i, token_i in enumerate(token_groups[0]):\n",
    "        for j, token_j in enumerate(token_groups[1]):\n",
    "            if token_i.startswith(\"<\") or token_j.startswith(\"<\"):\n",
    "                continue\n",
    "\n",
    "            if token_i == token_j:\n",
    "                indices.append((i, j))\n",
    "\n",
    "    return list(zip(*indices))\n",
    "\n",
    "\n",
    "def get_ignore_indices(prompts: list[str]) -> tuple[list[str], list[int]]:\n",
    "    tokens_a, tokens_b = prompts[0].split(), prompts[1].split()\n",
    "    if len(tokens_a) != len(tokens_b):\n",
    "        raise NotImplementedError(\"Different prompt lengths\")\n",
    "\n",
    "    index = tokens_b.index(\"<IGNORE>\")\n",
    "    word = tokens_a[index]\n",
    "\n",
    "    return [prompts[0], prompts[0]], get_replacement_indices([prompts[0], prompts[0]], word, word)[\n",
    "        0\n",
    "    ]\n",
    "\n",
    "\n",
    "def run_and_display(prompts, controller=None, seed=0, audio_length=10.0, save_results=False):\n",
    "    max_new_tokens = 2 ** round(np.log2(audio_length * model_proxy.frame_rate))\n",
    "\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    if controller is None:\n",
    "        controller = EmptyController()\n",
    "\n",
    "    controller.reset()\n",
    "    controller.batch_size = len(prompts)\n",
    "    controller.max_new_tokens = max_new_tokens\n",
    "\n",
    "    register_attention_control(model_proxy, controller)\n",
    "\n",
    "    inputs = model_proxy.encode(prompts)\n",
    "    audio_values = model_proxy.generate(inputs, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    if save_results:\n",
    "        output_folder = construct_output_folder_path(controller)\n",
    "        output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for prompt, audio_value in zip(prompts, audio_values):\n",
    "            filename = f\"{prompt}.wav\"\n",
    "            filepath = output_folder / filename\n",
    "            write(filepath, rate=model_proxy.sampling_rate, data=audio_value)\n",
    "\n",
    "    return audio_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Comparing self-attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ClapModel\n",
    "\n",
    "clap_model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "clap_processor = AutoProcessor.from_pretrained(\"laion/clap-htsat-unfused\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def cosine_similarity(prompt, audios, sr=48000):\n",
    "    # Resample audios\n",
    "    audios = np.stack(\n",
    "        [\n",
    "            librosa.resample(audio, orig_sr=model_proxy.sampling_rate, target_sr=sr)\n",
    "            for audio in audios\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    inputs = clap_processor(\n",
    "        text=prompt, audios=audios, return_tensors=\"pt\", sampling_rate=sr, padding=True\n",
    "    )\n",
    "\n",
    "    # Process prompt and audios\n",
    "    prompt_features = clap_model.get_text_features(\n",
    "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n",
    "    )\n",
    "    audio_features = clap_model.get_audio_features(\n",
    "        input_features=inputs[\"input_features\"], attention_mask=inputs[\"attention_mask\"]\n",
    "    )\n",
    "\n",
    "    # Calculate cosine similarity between audios\n",
    "    audio_audio_similarity = F.cosine_similarity(audio_features[0], audio_features[1], dim=0)\n",
    "\n",
    "    # Calculate cosine similarity between prompt at index 1 and audio at index 1\n",
    "    text_audio_similarity = F.cosine_similarity(prompt_features[0], audio_features[1], dim=0)\n",
    "\n",
    "    return audio_audio_similarity.item(), text_audio_similarity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def extract_pitch_classes(audio, sr=model_proxy.sampling_rate, hop_length=512):\n",
    "    # Extract pitch using librosa's piptrack function\n",
    "    _, magnitudes = librosa.core.piptrack(y=audio, sr=sr, hop_length=hop_length)\n",
    "\n",
    "    # Get the pitch with the maximum magnitude for each frame\n",
    "    pitch_classes = np.argmax(magnitudes, axis=0)\n",
    "\n",
    "    return pitch_classes\n",
    "\n",
    "\n",
    "def calculate_melody_accuracy(input_melody, generated_melody):\n",
    "    # Extract pitch classes from both melodies\n",
    "    input_pitch_classes = extract_pitch_classes(input_melody)\n",
    "    generated_pitch_classes = extract_pitch_classes(generated_melody)\n",
    "\n",
    "    # Calculate melody accuracy\n",
    "    accuracy = accuracy_score(input_pitch_classes, generated_pitch_classes)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_beat_consistency_score(audio, sr=model_proxy.sampling_rate):\n",
    "    # Beat detection\n",
    "    _, beat_frames = librosa.beat.beat_track(y=audio, sr=sr)\n",
    "\n",
    "    # Calculate Inter-Beat Intervals (IBIs)\n",
    "    ibis = np.diff(librosa.frames_to_time(beat_frames, sr=sr))\n",
    "\n",
    "    # Calculate mean and standard deviation of IBIs\n",
    "    mean_ibi = np.mean(ibis)\n",
    "    std_ibi = np.std(ibis)\n",
    "\n",
    "    # Calculate Beat Consistency Score (coefficient of variation)\n",
    "    beat_consistency_score = std_ibi / mean_ibi\n",
    "\n",
    "    return beat_consistency_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_snr(y):\n",
    "    # Calculate the power of the signal\n",
    "    signal_power = np.sum(y**2)\n",
    "\n",
    "    # Estimate the noise using spectral flatness\n",
    "    flatness = librosa.feature.spectral_flatness(y=y)\n",
    "\n",
    "    # Calculate the noise power\n",
    "    noise_power = np.sum(flatness)\n",
    "\n",
    "    # Calculate SNR in decibels (dB)\n",
    "    snr = 10 * np.log10(signal_power / noise_power)\n",
    "\n",
    "    return snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "\n",
    "def calculate_ssi(audios):\n",
    "    # Calculate spectrograms\n",
    "    specgram_1 = librosa.amplitude_to_db(np.abs(librosa.stft(audios[0])), ref=np.max)\n",
    "    specgram_2 = librosa.amplitude_to_db(np.abs(librosa.stft(audios[1])), ref=np.max)\n",
    "\n",
    "    # Normalize the spectrograms to [0, 1]\n",
    "    specgram_1 = (specgram_1 - np.min(specgram_1)) / (np.max(specgram_1) - np.min(specgram_1))\n",
    "    specgram_2 = (specgram_2 - np.min(specgram_2)) / (np.max(specgram_2) - np.min(specgram_2))\n",
    "\n",
    "    # Calculate Structural Similarity Index (SSI)\n",
    "    ssi_index, _ = ssim(specgram_1, specgram_2, data_range=1.0, full=True)\n",
    "\n",
    "    return ssi_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        samples: list[tuple[str, tuple[str, str]]],\n",
    "        soft_blending: bool = False,\n",
    "        seeds: Optional[list[int]] = None,\n",
    "    ) -> None:\n",
    "        if seeds is None:\n",
    "            seeds = [0, 1]\n",
    "\n",
    "        self.entries = []\n",
    "        for edit_type, prompts in samples:\n",
    "            controllers = []\n",
    "            if \"Ignore\" in edit_type:\n",
    "                prompts, indices = get_ignore_indices(prompts)\n",
    "                controllers = [IgnoreWordController(indices)]\n",
    "            elif \"Replace\" in edit_type:\n",
    "                words_a, words_b = prompts[0].split(), prompts[1].split()\n",
    "                index = next(\n",
    "                    (\n",
    "                        i\n",
    "                        for i, (word_a, word_b) in enumerate(zip(words_a, words_b))\n",
    "                        if word_a != word_b\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "                indices = get_replacement_indices(prompts, words_a[index], words_b[index])\n",
    "                controllers = [\n",
    "                    ReplaceWordController(indices, blend) for blend in np.arange(0.3, 0.8, 0.2)\n",
    "                ]\n",
    "            else:\n",
    "                raise NotImplementedError(f\"{edit_type} is not supported\")\n",
    "\n",
    "            if soft_blending:\n",
    "                controllers = map(SelfAttentionLerpControllerModifier, controllers)\n",
    "\n",
    "            for controller in controllers:\n",
    "                for seed in seeds:\n",
    "                    self.entries.append((edit_type, (prompts[0], prompts[1]), controller, seed))\n",
    "\n",
    "    def __iter__(self) -> Iterable[tuple[str, tuple[str, str], BaseController]]:\n",
    "        yield from self.entries\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "class CheckpointManager(object):\n",
    "    def __init__(self, filepath: Path) -> None:\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def load(self) -> dict[str, Any]:\n",
    "        if self.filepath.is_file():\n",
    "            with self.filepath.open(\"rb\") as file:\n",
    "                return pickle.load(file)\n",
    "\n",
    "        return {}\n",
    "\n",
    "    def dump(self, **data: dict[str, Any]) -> None:\n",
    "        with self.filepath.open(\"wb\") as file:\n",
    "            return pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    (\"Ignore\", (\"pop song with guitar and drums\", \"pop song with <IGNORE> and drums\")),\n",
    "    (\"Replace\", (\"pop song with guitar and drums\", \"pop song with synth and drums\")),\n",
    "    (\"Replace (Sentiment)\", (\"happy pop song\", \"sad pop song\")),\n",
    "    (\"Replace (Chord)\", (\"a major chord pop song\", \"a minor chord pop song\")),\n",
    "    (\"Ignore\", (\"rock ballad with piano\", \"rock ballad with <IGNORE>\")),\n",
    "    (\n",
    "        \"Replace\",\n",
    "        (\"jazz ensemble with trumpet and saxophone\", \"jazz ensemble with piano and saxophone\"),\n",
    "    ),\n",
    "    (\"Replace (Sentiment)\", (\"energetic electronic dance track\", \"calm electronic dance track\")),\n",
    "    (\n",
    "        \"Replace (Chord)\",\n",
    "        (\"blues riff in E on electric guitar\", \"blues riff in G on electric guitar\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Ignore\",\n",
    "        (\n",
    "            \"acoustic folk song with banjo and harmonica\",\n",
    "            \"acoustic folk song with <IGNORE> and harmonica\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Replace\",\n",
    "        (\n",
    "            \"classical symphony with violins and cellos\",\n",
    "            \"classical symphony with flutes and cellos\",\n",
    "        ),\n",
    "    ),\n",
    "    (\"Replace (Sentiment)\", (\"upbeat indie pop anthem\", \"melancholic indie pop anthem\")),\n",
    "    (\"Replace (Chord)\", (\"piano sonata in C minor\", \"piano sonata in A minor\")),\n",
    "    (\"Ignore\", (\"funky bassline with slap technique\", \"funky <IGNORE> with slap technique\")),\n",
    "    (\n",
    "        \"Replace\",\n",
    "        (\"latin jazz fusion with congas and bongos\", \"latin jazz fusion with timbales and bongos\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Replace (Sentiment)\",\n",
    "        (\"motivational corporate background music\", \"relaxing corporate background music\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Replace (Chord)\",\n",
    "        (\n",
    "            \"gospel choir with dominant seventh chords\",\n",
    "            \"gospel choir with diminished seventh chords\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Ignore\",\n",
    "        (\n",
    "            \"ambient electronic soundscape with synthesizers\",\n",
    "            \"ambient electronic <IGNORE> with synthesizers\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Replace\",\n",
    "        (\n",
    "            \"orchestral film score with strings and brass\",\n",
    "            \"orchestral film score with woodwinds and brass\",\n",
    "        ),\n",
    "    ),\n",
    "    (\"Replace (Sentiment)\", (\"uplifting reggae vibes\", \"heartbreaking reggae vibes\")),\n",
    "    (\n",
    "        \"Replace (Chord)\",\n",
    "        (\"punk rock anthem with power chords\", \"punk rock anthem with barre chords\"),\n",
    "    ),\n",
    "]\n",
    "\n",
    "dataset = Dataset(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def run_greedy_ablation_study(checkpoint_path: Optional[Path] = None):\n",
    "    if checkpoint_path is None:\n",
    "        checkpoint_path = RESULTS_DIR / \"greedy_checkpoint.pkl\"\n",
    "\n",
    "    checkpoint_manager = CheckpointManager(checkpoint_path)\n",
    "\n",
    "    columns = [\n",
    "        \"Edit\",\n",
    "        \"Layers\",\n",
    "        \"Source Prompt\",\n",
    "        \"Editted Prompt\",\n",
    "        \"Source Audio\",\n",
    "        \"Editted Audio\",\n",
    "        \"Text-Audio Cosine Similarity\",\n",
    "        \"Audio-Audio Cosine Similarity\",\n",
    "    ]\n",
    "\n",
    "    prompts = [samples[0][1][0], samples[0][1][1]]\n",
    "    controller = AttentionStore()\n",
    "    audio_values = run_and_display(prompts, controller)\n",
    "\n",
    "    # !This is a heuristic\n",
    "    sorted_indices = controller.get_self_attention_importance()[0].tolist()\n",
    "\n",
    "    cross_attention_layer_indices = [2 * (i + 1) for i in range(len(model_proxy.decoder_layers))]\n",
    "    error_threshold = 0.1\n",
    "\n",
    "    checkpoint = checkpoint_manager.load()\n",
    "    black_listed_indices = checkpoint.get(\"black_listed_indices\", [])\n",
    "    visited_indices = checkpoint.get(\"visited_indices\", [])\n",
    "    df_list = checkpoint.get(\"df_list\", [])\n",
    "\n",
    "    all_indices = [\n",
    "        i for i in sorted_indices if i not in visited_indices and i not in black_listed_indices\n",
    "    ]\n",
    "    progress_bar_a = tqdm(all_indices, position=0)\n",
    "    for iteration in progress_bar_a:\n",
    "        indices = [\n",
    "            i for i in all_indices if i not in visited_indices and i not in black_listed_indices\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            previous_max_score = df_list[-1][\"Score\"].item()\n",
    "        except IndexError:\n",
    "            previous_max_score = 0\n",
    "\n",
    "        progress_bar_b, scores = tqdm(indices, position=1, leave=False), []\n",
    "        for index in progress_bar_b:\n",
    "            self_attention_layer_indices = [2 * i + 1 for i in [index, *visited_indices]]\n",
    "\n",
    "            progress_bar_c, df_list_ablation = tqdm(dataset, position=2, leave=False), []\n",
    "            for edit, prompts, controller, seed in progress_bar_c:\n",
    "                layers = \",\".join(f\"{x:02d}\" for x in self_attention_layer_indices).strip()\n",
    "\n",
    "                progress_bar_c.set_postfix({\"layers\": layers})\n",
    "\n",
    "                attention_layer_indices = [\n",
    "                    *cross_attention_layer_indices,\n",
    "                    *self_attention_layer_indices,\n",
    "                ]\n",
    "                controller = DecoderLayerControllerModifier(\n",
    "                    controller, set(attention_layer_indices)\n",
    "                )\n",
    "                audio_values = run_and_display(prompts, controller, seed=seed)\n",
    "\n",
    "                audio_audio_similarity, text_audio_similarity = cosine_similarity(\n",
    "                    prompts[1], audio_values\n",
    "                )\n",
    "\n",
    "                row = [edit, layers, prompts[0], prompts[1], audio_values[0], audio_values[1]]\n",
    "                row.append(text_audio_similarity)\n",
    "                row.append(audio_audio_similarity)\n",
    "\n",
    "                df_list_ablation.append(pd.DataFrame([row], columns=columns))\n",
    "\n",
    "            df = pd.concat(df_list_ablation, ignore_index=True)\n",
    "\n",
    "            metrics = df[[\"Text-Audio Cosine Similarity\", \"Audio-Audio Cosine Similarity\"]]\n",
    "            score = metrics.mean(axis=None)\n",
    "            scores.append(score)\n",
    "\n",
    "            error = abs(score - previous_max_score)\n",
    "            if score < previous_max_score and error > error_threshold:\n",
    "                black_listed_indices.append(index)\n",
    "\n",
    "            progress_bar_b.set_postfix({\"index\": index, \"score\": f\"{score:.3f}\"})\n",
    "\n",
    "        max_score = max(scores)\n",
    "\n",
    "        error = abs(max_score - previous_max_score)\n",
    "        if max_score < previous_max_score and error > error_threshold:\n",
    "            break\n",
    "\n",
    "        max_score_index = indices[scores.index(max_score)]\n",
    "        visited_indices.append(max_score_index)\n",
    "        df_list.append(\n",
    "            pd.DataFrame([[max_score, visited_indices.copy()]], columns=[\"Score\", \"Indices\"])\n",
    "        )\n",
    "\n",
    "        checkpoint_manager.dump(\n",
    "            visited_indices=visited_indices,\n",
    "            black_listed_indices=black_listed_indices,\n",
    "            df_list=df_list,\n",
    "        )\n",
    "\n",
    "        progress_bar_a.set_postfix(\n",
    "            {\n",
    "                \"last_checkpoint\": f\"{iteration:02d}\",\n",
    "                \"current\": f\"{max_score:.3f}\",\n",
    "                \"previous\": f\"{previous_max_score:.3f}\",\n",
    "                \"error\": f\"{error * 100:.2f}%\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    df.to_pickle(RESULTS_DIR / \"greedy.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_greedy_ablation_study()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ablation_study(dataset, self_attention_layer_groups):\n",
    "    columns = [\n",
    "        \"Edit\",\n",
    "        \"Layers\",\n",
    "        \"Source Prompt\",\n",
    "        \"Editted Prompt\",\n",
    "        \"Source Audio\",\n",
    "        \"Editted Audio\",\n",
    "        \"Text-Audio Cosine Similarity\",\n",
    "        \"Audio-Audio Cosine Similarity\",\n",
    "        \"Melody Accuracy\",\n",
    "        \"Beat Consistency Score\",\n",
    "        \"Signal to Noise Ratio\",\n",
    "        \"Structural Similarity Index\",\n",
    "    ]\n",
    "\n",
    "    df_list = []\n",
    "    for edit, prompts, controller, seed in tqdm(dataset, position=0):\n",
    "        for self_attention_layers in tqdm(self_attention_layer_groups, leave=False, position=1):\n",
    "            controller = DecoderLayerControllerModifier(\n",
    "                controller,\n",
    "                set(\n",
    "                    [2 * (i + 1) for i in range(len(model_proxy.decoder_layers))]\n",
    "                    + self_attention_layers\n",
    "                ),\n",
    "            )\n",
    "            audio_values = run_and_display(prompts, controller, seed=seed)\n",
    "\n",
    "            layers = \",\".join(f\"{x:02d}\" for x in self_attention_layers).strip()\n",
    "\n",
    "            audio_audio_similarity, text_audio_similarity = cosine_similarity(\n",
    "                prompts[1], audio_values\n",
    "            )\n",
    "\n",
    "            row = [edit, layers, prompts[0], prompts[1], audio_values[0], audio_values[1]]\n",
    "            row.append(text_audio_similarity)\n",
    "            row.append(audio_audio_similarity)\n",
    "            row.append(calculate_melody_accuracy(audio_values[0], audio_values[1]))\n",
    "            row.append(-calculate_beat_consistency_score(audio_values[1]))\n",
    "            row.append(calculate_snr(audio_values[1]))\n",
    "            row.append(calculate_ssi(audio_values))\n",
    "\n",
    "            df_list.append(pd.DataFrame([row], columns=columns))\n",
    "\n",
    "    return pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "#### Comparing individual self-attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention_layer_groups = [[2 * x + 1] for x in range(len(model_proxy.decoder_layers))]\n",
    "df = run_ablation_study(dataset, self_attention_layer_groups)\n",
    "df.to_pickle(RESULTS_DIR / \"individual_hard.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Comparing `n - 1` self-attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention_layer_groups = [\n",
    "    [2 * y + 1 for y in range(len(model_proxy.decoder_layers)) if x != y]\n",
    "    for x in range(len(model_proxy.decoder_layers))\n",
    "]\n",
    "df = run_ablation_study(dataset, self_attention_layer_groups)\n",
    "df.to_pickle(RESULTS_DIR / \"leave_one_out_hard.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "#### Comparing incremental groups of self-attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention_layer_groups = [\n",
    "    [2 * y + 1 for y in range(0, x)] for x in range(1, len(model_proxy.decoder_layers) + 1)\n",
    "]\n",
    "df = run_ablation_study(dataset, self_attention_layer_groups)\n",
    "df.to_pickle(RESULTS_DIR / \"incremental_hard.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "#### Comparing individual self-attention layers (Soft-blending self-attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(samples, soft_blending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention_layer_groups = [[2 * x + 1] for x in range(len(model_proxy.decoder_layers))]\n",
    "df = run_ablation_study(dataset, self_attention_layer_groups)\n",
    "df.to_pickle(RESULTS_DIR / \"individual_soft.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "#### Comparing `n - 1` self-attention layers (Soft-blending self-attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention_layer_groups = [\n",
    "    [2 * y + 1 for y in range(len(model_proxy.decoder_layers)) if x != y]\n",
    "    for x in range(len(model_proxy.decoder_layers))\n",
    "]\n",
    "df = run_ablation_study(dataset, self_attention_layer_groups)\n",
    "df.to_pickle(RESULTS_DIR / \"leave_one_out_soft.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "#### Comparing incremental groups of self-attention layers (Soft-blending self-attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention_layer_groups = [[2 * y + 1 for y in range(0, x)] for x in range(1, 49)]\n",
    "df = run_ablation_study(dataset, self_attention_layer_groups)\n",
    "df.to_pickle(RESULTS_DIR / \"incremental_soft.pkl\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
