{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "70300319-d206-43ce-b3bf-3da6b079f20f"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "e70e6dbb-3211-4ef9-93f6-efaba764ac77"
   },
   "source": [
    "## Prepare the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "MODEL_NAME = \"facebook/musicgen-small\"\n",
    "\n",
    "if (Path(\"/\") / \"home\" / \"vsioros\").is_dir():\n",
    "    BASE_DIR = Path(\"/\") / \"home\" / \"vsioros\"\n",
    "    MODEL_NAME = \"facebook/musicgen-large\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "77ee39cc-654b-4f0e-b601-013e484c16f0"
   },
   "source": [
    "## Load the Model\n",
    "\n",
    "The pre-trained MusicGen small, medium and large checkpoints can be loaded from the [pre-trained weights](https://huggingface.co/models?search=facebook/musicgen-) on the Hugging Face Hub. Change the repo id with the checkpoint size you wish to load. We'll default to the small checkpoint, which is the fastest of the three but has the lowest audio quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from editgen import EditGenPipeline\n",
    "\n",
    "pipeline = EditGenPipeline(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Ignoring a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "from editgen.controllers import IgnoreWordController\n",
    "from editgen.modifiers import SelfAttentionLerpControllerModifier\n",
    "\n",
    "prompts = [\"accoustic guitar solo\", \"<IGNORE> guitar solo\"]\n",
    "prompts, controller = IgnoreWordController.from_prompts(pipeline, prompts)\n",
    "controller = SelfAttentionLerpControllerModifier(controller)\n",
    "\n",
    "audio_values = pipeline(prompts, controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[0], rate=pipeline.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[1], rate=pipeline.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Replacement edit with Prompt-to-Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"pop song with guitar and drums\",\n",
    "    \"pop song with synth and drums\",\n",
    "]\n",
    "# controller = DecoderLayerControllerModifier(ReplaceWordController(3, 0.3), set([2 * i for i in range(0, 49)] + [1]))\n",
    "# controller = DecoderLayerControllerModifier(ReplaceWordController(3, 0.3), {0,1,2,3,4,5,6,7,8,9,10,11,12,14,15,16,17,18,19,20,21,24,33,34})\n",
    "controller = AttentionLerpControllerModifier(\n",
    "    ReplaceWordController(get_replacement_indices(prompts, \"guitar\", \"synth\"), 1)\n",
    ")\n",
    "# controller = ReplaceWordController(3, 0.3)\n",
    "audio_values = run_and_display(prompts, controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[0], rate=model_proxy.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[1], rate=model_proxy.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Refinement edit with Prompt-to-Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"heavy guitar solo\",\n",
    "    \"heavy guitar and drums solo\",\n",
    "]\n",
    "controller = AttentionLerpControllerModifier(\n",
    "    RefineController(get_refine_word_indices(prompts), 1)\n",
    ")\n",
    "audio_values = run_and_display(prompts, controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[0], rate=model_proxy.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[1], rate=model_proxy.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Reweight edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"heavy guitar solo\",\n",
    "    \"heavy guitar solo\",\n",
    "]\n",
    "controller = AttentionCutoffControllerModifier(\n",
    "    ReweightWordController(get_reweight_word_indices(prompts, \"heavy\"), 2)\n",
    ")\n",
    "audio_values = run_and_display(prompts, controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[0], rate=model_proxy.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[1], rate=model_proxy.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "#### Sentiment replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"happy pop song\",\n",
    "    \"sad pop song\",\n",
    "]\n",
    "# controller = DecoderLayerControllerModifier(ReplaceWordController(3, 0.3), set([2 * i for i in range(0, 49)] + [1]))\n",
    "# controller = DecoderLayerControllerModifier(ReplaceWordController(3, 0.3), {0,1,2,3,4,5,6,7,8,9,10,11,12,14,15,16,17,18,19,20,21,24,33,34})\n",
    "controller = SelfAttentionLerpControllerModifier(\n",
    "    ReplaceWordController(get_replacement_indices(prompts, \"happy\", \"sad\"), 0.3)\n",
    ")\n",
    "# controller = ReplaceWordController(3, 0.3)\n",
    "audio_values = run_and_display(prompts, controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[0], rate=model_proxy.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[1], rate=model_proxy.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "#### Majore vs Minore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"a major chord pop song\",\n",
    "    \"a minor chord pop song\",\n",
    "]\n",
    "# controller = DecoderLayerControllerModifier(ReplaceWordController(3, 0.3), set([2 * i for i in range(0, 49)] + [1]))\n",
    "# controller = DecoderLayerControllerModifier(ReplaceWordController(3, 0.3), {0,1,2,3,4,5,6,7,8,9,10,11,12,14,15,16,17,18,19,20,21,24,33,34})\n",
    "controller = SelfAttentionLerpControllerModifier(\n",
    "    ReplaceWordController(get_replacement_indices(prompts, \"major\", \"minor\"), 0.3)\n",
    ")\n",
    "# controller = ReplaceWordController(3, 0.3)\n",
    "audio_values = run_and_display(prompts, controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[0], rate=model_proxy.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[1], rate=model_proxy.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Injecting Random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"pop song with guitar and drums\",\n",
    "    \"pop song with synth and drums\",\n",
    "]\n",
    "controller = OffsetControllerModifier(\n",
    "    SelfAttentionLerpControllerModifier(\n",
    "        ReplaceWordController(get_replacement_indices(prompts, \"guitar\", \"synth\"), 0.3)\n",
    "    ),\n",
    "    offset=0.5,\n",
    ")\n",
    "audio_values = run_and_display(prompts, controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[0], rate=model_proxy.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[1], rate=model_proxy.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Ablation Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Counter-factual Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"a classical song with guitar\",\n",
    "    \"a classical song with violin\",\n",
    "]\n",
    "controller = SelfAttentionLerpControllerModifier(\n",
    "    ReplaceWordController(get_replacement_indices(prompts, \"guitar\", \"violin\"), 0.3)\n",
    ")\n",
    "audio_values = run_and_display(prompts, controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[0], rate=model_proxy.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[1], rate=model_proxy.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Comparing self-attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ClapModel\n",
    "\n",
    "clap_model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "clap_processor = AutoProcessor.from_pretrained(\"laion/clap-htsat-unfused\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def cosine_similarity(prompt, audios, sr=48000):\n",
    "    # Resample audios\n",
    "    audios = np.stack(\n",
    "        [\n",
    "            librosa.resample(audio, orig_sr=model_proxy.sampling_rate, target_sr=sr)\n",
    "            for audio in audios\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    inputs = clap_processor(\n",
    "        text=prompt, audios=audios, return_tensors=\"pt\", sampling_rate=sr, padding=True\n",
    "    )\n",
    "\n",
    "    # Process prompt and audios\n",
    "    prompt_features = clap_model.get_text_features(\n",
    "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n",
    "    )\n",
    "    audio_features = clap_model.get_audio_features(\n",
    "        input_features=inputs[\"input_features\"], attention_mask=inputs[\"attention_mask\"]\n",
    "    )\n",
    "\n",
    "    # Calculate cosine similarity between audios\n",
    "    audio_audio_similarity = F.cosine_similarity(\n",
    "        audio_features[0], audio_features[1], dim=0\n",
    "    )\n",
    "\n",
    "    # Calculate cosine similarity between prompt at index 1 and audio at index 1\n",
    "    text_audio_similarity = F.cosine_similarity(\n",
    "        prompt_features[0], audio_features[1], dim=0\n",
    "    )\n",
    "\n",
    "    return audio_audio_similarity.item(), text_audio_similarity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def extract_pitch_classes(audio, sr=model_proxy.sampling_rate, hop_length=512):\n",
    "    # Extract pitch using librosa's piptrack function\n",
    "    _, magnitudes = librosa.core.piptrack(y=audio, sr=sr, hop_length=hop_length)\n",
    "\n",
    "    # Get the pitch with the maximum magnitude for each frame\n",
    "    pitch_classes = np.argmax(magnitudes, axis=0)\n",
    "\n",
    "    return pitch_classes\n",
    "\n",
    "\n",
    "def calculate_melody_accuracy(input_melody, generated_melody):\n",
    "    # Extract pitch classes from both melodies\n",
    "    input_pitch_classes = extract_pitch_classes(input_melody)\n",
    "    generated_pitch_classes = extract_pitch_classes(generated_melody)\n",
    "\n",
    "    # Calculate melody accuracy\n",
    "    accuracy = accuracy_score(input_pitch_classes, generated_pitch_classes)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_beat_consistency_score(audio, sr=model_proxy.sampling_rate):\n",
    "    # Beat detection\n",
    "    _, beat_frames = librosa.beat.beat_track(y=audio, sr=sr)\n",
    "\n",
    "    # Calculate Inter-Beat Intervals (IBIs)\n",
    "    ibis = np.diff(librosa.frames_to_time(beat_frames, sr=sr))\n",
    "\n",
    "    # Calculate mean and standard deviation of IBIs\n",
    "    mean_ibi = np.mean(ibis)\n",
    "    std_ibi = np.std(ibis)\n",
    "\n",
    "    # Calculate Beat Consistency Score (coefficient of variation)\n",
    "    beat_consistency_score = std_ibi / mean_ibi\n",
    "\n",
    "    return beat_consistency_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_snr(y):\n",
    "    # Calculate the power of the signal\n",
    "    signal_power = np.sum(y**2)\n",
    "\n",
    "    # Estimate the noise using spectral flatness\n",
    "    flatness = librosa.feature.spectral_flatness(y=y)\n",
    "\n",
    "    # Calculate the noise power\n",
    "    noise_power = np.sum(flatness)\n",
    "\n",
    "    # Calculate SNR in decibels (dB)\n",
    "    snr = 10 * np.log10(signal_power / noise_power)\n",
    "\n",
    "    return snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "\n",
    "def calculate_ssi(audios):\n",
    "    # Calculate spectrograms\n",
    "    specgram_1 = librosa.amplitude_to_db(np.abs(librosa.stft(audios[0])), ref=np.max)\n",
    "    specgram_2 = librosa.amplitude_to_db(np.abs(librosa.stft(audios[1])), ref=np.max)\n",
    "\n",
    "    # Normalize the spectrograms to [0, 1]\n",
    "    specgram_1 = (specgram_1 - np.min(specgram_1)) / (\n",
    "        np.max(specgram_1) - np.min(specgram_1)\n",
    "    )\n",
    "    specgram_2 = (specgram_2 - np.min(specgram_2)) / (\n",
    "        np.max(specgram_2) - np.min(specgram_2)\n",
    "    )\n",
    "\n",
    "    # Calculate Structural Similarity Index (SSI)\n",
    "    ssi_index, _ = ssim(specgram_1, specgram_2, data_range=1.0, full=True)\n",
    "\n",
    "    return ssi_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        samples: list[tuple[str, tuple[str, str]]],\n",
    "        soft_blending: bool = False,\n",
    "        seeds: Optional[list[int]] = None,\n",
    "    ) -> None:\n",
    "        if seeds is None:\n",
    "            seeds = [0, 1]\n",
    "\n",
    "        self.entries = []\n",
    "        for edit_type, prompts in samples:\n",
    "            controllers = []\n",
    "            if \"Ignore\" in edit_type:\n",
    "                prompts, indices = get_ignore_indices(prompts)\n",
    "                controllers = [IgnoreWordController(indices)]\n",
    "            elif \"Replace\" in edit_type:\n",
    "                words_a, words_b = prompts[0].split(), prompts[1].split()\n",
    "                index = next(\n",
    "                    (\n",
    "                        i\n",
    "                        for i, (word_a, word_b) in enumerate(zip(words_a, words_b))\n",
    "                        if word_a != word_b\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "                indices = get_replacement_indices(\n",
    "                    prompts, words_a[index], words_b[index]\n",
    "                )\n",
    "                controllers = [\n",
    "                    ReplaceWordController(indices, blend)\n",
    "                    for blend in np.arange(0.3, 0.8, 0.2)\n",
    "                ]\n",
    "            else:\n",
    "                raise NotImplementedError(f\"{edit_type} is not supported\")\n",
    "\n",
    "            if soft_blending:\n",
    "                controllers = map(SelfAttentionLerpControllerModifier, controllers)\n",
    "\n",
    "            for controller in controllers:\n",
    "                for seed in seeds:\n",
    "                    self.entries.append(\n",
    "                        (edit_type, (prompts[0], prompts[1]), controller, seed)\n",
    "                    )\n",
    "\n",
    "    def __iter__(self) -> Iterable[tuple[str, tuple[str, str], BaseController]]:\n",
    "        yield from self.entries\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "class CheckpointManager(object):\n",
    "    def __init__(self, filepath: Path) -> None:\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def load(self) -> dict[str, Any]:\n",
    "        if self.filepath.is_file():\n",
    "            with self.filepath.open(\"rb\") as file:\n",
    "                return pickle.load(file)\n",
    "\n",
    "        return {}\n",
    "\n",
    "    def dump(self, **data: dict[str, Any]) -> None:\n",
    "        with self.filepath.open(\"wb\") as file:\n",
    "            return pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    (\"Ignore\", (\"pop song with guitar and drums\", \"pop song with <IGNORE> and drums\")),\n",
    "    (\"Replace\", (\"pop song with guitar and drums\", \"pop song with synth and drums\")),\n",
    "    (\"Replace (Sentiment)\", (\"happy pop song\", \"sad pop song\")),\n",
    "    (\"Replace (Chord)\", (\"a major chord pop song\", \"a minor chord pop song\")),\n",
    "    (\"Ignore\", (\"rock ballad with piano\", \"rock ballad with <IGNORE>\")),\n",
    "    (\n",
    "        \"Replace\",\n",
    "        (\n",
    "            \"jazz ensemble with trumpet and saxophone\",\n",
    "            \"jazz ensemble with piano and saxophone\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Replace (Sentiment)\",\n",
    "        (\"energetic electronic dance track\", \"calm electronic dance track\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Replace (Chord)\",\n",
    "        (\"blues riff in E on electric guitar\", \"blues riff in G on electric guitar\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Ignore\",\n",
    "        (\n",
    "            \"acoustic folk song with banjo and harmonica\",\n",
    "            \"acoustic folk song with <IGNORE> and harmonica\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Replace\",\n",
    "        (\n",
    "            \"classical symphony with violins and cellos\",\n",
    "            \"classical symphony with flutes and cellos\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Replace (Sentiment)\",\n",
    "        (\"upbeat indie pop anthem\", \"melancholic indie pop anthem\"),\n",
    "    ),\n",
    "    (\"Replace (Chord)\", (\"piano sonata in C minor\", \"piano sonata in A minor\")),\n",
    "    (\n",
    "        \"Ignore\",\n",
    "        (\"funky bassline with slap technique\", \"funky <IGNORE> with slap technique\"),\n",
    "    ),\n",
    "    (\n",
    "        \"Replace\",\n",
    "        (\n",
    "            \"latin jazz fusion with congas and bongos\",\n",
    "            \"latin jazz fusion with timbales and bongos\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Replace (Sentiment)\",\n",
    "        (\n",
    "            \"motivational corporate background music\",\n",
    "            \"relaxing corporate background music\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Replace (Chord)\",\n",
    "        (\n",
    "            \"gospel choir with dominant seventh chords\",\n",
    "            \"gospel choir with diminished seventh chords\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Ignore\",\n",
    "        (\n",
    "            \"ambient electronic soundscape with synthesizers\",\n",
    "            \"ambient electronic <IGNORE> with synthesizers\",\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Replace\",\n",
    "        (\n",
    "            \"orchestral film score with strings and brass\",\n",
    "            \"orchestral film score with woodwinds and brass\",\n",
    "        ),\n",
    "    ),\n",
    "    (\"Replace (Sentiment)\", (\"uplifting reggae vibes\", \"heartbreaking reggae vibes\")),\n",
    "    (\n",
    "        \"Replace (Chord)\",\n",
    "        (\"punk rock anthem with power chords\", \"punk rock anthem with barre chords\"),\n",
    "    ),\n",
    "]\n",
    "\n",
    "dataset = Dataset(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def run_greedy_ablation_study(checkpoint_path: Optional[Path] = None):\n",
    "    if checkpoint_path is None:\n",
    "        checkpoint_path = RESULTS_DIR / \"greedy_checkpoint.pkl\"\n",
    "\n",
    "    checkpoint_manager = CheckpointManager(checkpoint_path)\n",
    "\n",
    "    columns = [\n",
    "        \"Edit\",\n",
    "        \"Layers\",\n",
    "        \"Source Prompt\",\n",
    "        \"Editted Prompt\",\n",
    "        \"Source Audio\",\n",
    "        \"Editted Audio\",\n",
    "        \"Text-Audio Cosine Similarity\",\n",
    "        \"Audio-Audio Cosine Similarity\",\n",
    "    ]\n",
    "\n",
    "    prompts = [samples[0][1][0], samples[0][1][1]]\n",
    "    controller = AttentionStore()\n",
    "    audio_values = run_and_display(prompts, controller)\n",
    "\n",
    "    # !This is a heuristic\n",
    "    sorted_indices = controller.get_self_attention_importance()[0].tolist()\n",
    "\n",
    "    cross_attention_layer_indices = [\n",
    "        2 * (i + 1) for i in range(len(model_proxy.decoder_layers))\n",
    "    ]\n",
    "    error_threshold = 0.1\n",
    "\n",
    "    checkpoint = checkpoint_manager.load()\n",
    "    black_listed_indices = checkpoint.get(\"black_listed_indices\", [])\n",
    "    visited_indices = checkpoint.get(\"visited_indices\", [])\n",
    "    df_list = checkpoint.get(\"df_list\", [])\n",
    "\n",
    "    all_indices = [\n",
    "        i\n",
    "        for i in sorted_indices\n",
    "        if i not in visited_indices and i not in black_listed_indices\n",
    "    ]\n",
    "    progress_bar_a = tqdm(all_indices, position=0)\n",
    "    for iteration in progress_bar_a:\n",
    "        indices = [\n",
    "            i\n",
    "            for i in all_indices\n",
    "            if i not in visited_indices and i not in black_listed_indices\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            previous_max_score = df_list[-1][\"Score\"].item()\n",
    "        except IndexError:\n",
    "            previous_max_score = 0\n",
    "\n",
    "        progress_bar_b, scores = tqdm(indices, position=1, leave=False), []\n",
    "        for index in progress_bar_b:\n",
    "            self_attention_layer_indices = [\n",
    "                2 * i + 1 for i in [index, *visited_indices]\n",
    "            ]\n",
    "\n",
    "            progress_bar_c, df_list_ablation = (\n",
    "                tqdm(dataset, position=2, leave=False),\n",
    "                [],\n",
    "            )\n",
    "            for edit, prompts, controller, seed in progress_bar_c:\n",
    "                layers = \",\".join(\n",
    "                    f\"{x:02d}\" for x in self_attention_layer_indices\n",
    "                ).strip()\n",
    "\n",
    "                progress_bar_c.set_postfix({\"layers\": layers})\n",
    "\n",
    "                attention_layer_indices = [\n",
    "                    *cross_attention_layer_indices,\n",
    "                    *self_attention_layer_indices,\n",
    "                ]\n",
    "                controller = DecoderLayerControllerModifier(\n",
    "                    controller, set(attention_layer_indices)\n",
    "                )\n",
    "                audio_values = run_and_display(prompts, controller, seed=seed)\n",
    "\n",
    "                audio_audio_similarity, text_audio_similarity = cosine_similarity(\n",
    "                    prompts[1], audio_values\n",
    "                )\n",
    "\n",
    "                row = [\n",
    "                    edit,\n",
    "                    layers,\n",
    "                    prompts[0],\n",
    "                    prompts[1],\n",
    "                    audio_values[0],\n",
    "                    audio_values[1],\n",
    "                ]\n",
    "                row.append(text_audio_similarity)\n",
    "                row.append(audio_audio_similarity)\n",
    "\n",
    "                df_list_ablation.append(pd.DataFrame([row], columns=columns))\n",
    "\n",
    "            df = pd.concat(df_list_ablation, ignore_index=True)\n",
    "\n",
    "            metrics = df[\n",
    "                [\"Text-Audio Cosine Similarity\", \"Audio-Audio Cosine Similarity\"]\n",
    "            ]\n",
    "            score = metrics.mean(axis=None)\n",
    "            scores.append(score)\n",
    "\n",
    "            error = abs(score - previous_max_score)\n",
    "            if score < previous_max_score and error > error_threshold:\n",
    "                black_listed_indices.append(index)\n",
    "\n",
    "            progress_bar_b.set_postfix({\"index\": index, \"score\": f\"{score:.3f}\"})\n",
    "\n",
    "        max_score = max(scores)\n",
    "\n",
    "        error = abs(max_score - previous_max_score)\n",
    "        if max_score < previous_max_score and error > error_threshold:\n",
    "            break\n",
    "\n",
    "        max_score_index = indices[scores.index(max_score)]\n",
    "        visited_indices.append(max_score_index)\n",
    "        df_list.append(\n",
    "            pd.DataFrame(\n",
    "                [[max_score, visited_indices.copy()]], columns=[\"Score\", \"Indices\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        checkpoint_manager.dump(\n",
    "            visited_indices=visited_indices,\n",
    "            black_listed_indices=black_listed_indices,\n",
    "            df_list=df_list,\n",
    "        )\n",
    "\n",
    "        progress_bar_a.set_postfix(\n",
    "            {\n",
    "                \"last_checkpoint\": f\"{iteration:02d}\",\n",
    "                \"current\": f\"{max_score:.3f}\",\n",
    "                \"previous\": f\"{previous_max_score:.3f}\",\n",
    "                \"error\": f\"{error * 100:.2f}%\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    df.to_pickle(RESULTS_DIR / \"greedy.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_greedy_ablation_study()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ablation_study(dataset, self_attention_layer_groups):\n",
    "    columns = [\n",
    "        \"Edit\",\n",
    "        \"Layers\",\n",
    "        \"Source Prompt\",\n",
    "        \"Editted Prompt\",\n",
    "        \"Source Audio\",\n",
    "        \"Editted Audio\",\n",
    "        \"Text-Audio Cosine Similarity\",\n",
    "        \"Audio-Audio Cosine Similarity\",\n",
    "        \"Melody Accuracy\",\n",
    "        \"Beat Consistency Score\",\n",
    "        \"Signal to Noise Ratio\",\n",
    "        \"Structural Similarity Index\",\n",
    "    ]\n",
    "\n",
    "    df_list = []\n",
    "    for edit, prompts, controller, seed in tqdm(dataset, position=0):\n",
    "        for self_attention_layers in tqdm(\n",
    "            self_attention_layer_groups, leave=False, position=1\n",
    "        ):\n",
    "            controller = DecoderLayerControllerModifier(\n",
    "                controller,\n",
    "                set(\n",
    "                    [2 * (i + 1) for i in range(len(model_proxy.decoder_layers))]\n",
    "                    + self_attention_layers\n",
    "                ),\n",
    "            )\n",
    "            audio_values = run_and_display(prompts, controller, seed=seed)\n",
    "\n",
    "            layers = \",\".join(f\"{x:02d}\" for x in self_attention_layers).strip()\n",
    "\n",
    "            audio_audio_similarity, text_audio_similarity = cosine_similarity(\n",
    "                prompts[1], audio_values\n",
    "            )\n",
    "\n",
    "            row = [\n",
    "                edit,\n",
    "                layers,\n",
    "                prompts[0],\n",
    "                prompts[1],\n",
    "                audio_values[0],\n",
    "                audio_values[1],\n",
    "            ]\n",
    "            row.append(text_audio_similarity)\n",
    "            row.append(audio_audio_similarity)\n",
    "            row.append(calculate_melody_accuracy(audio_values[0], audio_values[1]))\n",
    "            row.append(-calculate_beat_consistency_score(audio_values[1]))\n",
    "            row.append(calculate_snr(audio_values[1]))\n",
    "            row.append(calculate_ssi(audio_values))\n",
    "\n",
    "            df_list.append(pd.DataFrame([row], columns=columns))\n",
    "\n",
    "    return pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "#### Comparing individual self-attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention_layer_groups = [\n",
    "    [2 * x + 1] for x in range(len(model_proxy.decoder_layers))\n",
    "]\n",
    "df = run_ablation_study(dataset, self_attention_layer_groups)\n",
    "df.to_pickle(RESULTS_DIR / \"individual_hard.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "#### Comparing `n - 1` self-attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention_layer_groups = [\n",
    "    [2 * y + 1 for y in range(len(model_proxy.decoder_layers)) if x != y]\n",
    "    for x in range(len(model_proxy.decoder_layers))\n",
    "]\n",
    "df = run_ablation_study(dataset, self_attention_layer_groups)\n",
    "df.to_pickle(RESULTS_DIR / \"leave_one_out_hard.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "#### Comparing incremental groups of self-attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention_layer_groups = [\n",
    "    [2 * y + 1 for y in range(0, x)]\n",
    "    for x in range(1, len(model_proxy.decoder_layers) + 1)\n",
    "]\n",
    "df = run_ablation_study(dataset, self_attention_layer_groups)\n",
    "df.to_pickle(RESULTS_DIR / \"incremental_hard.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "#### Comparing individual self-attention layers (Soft-blending self-attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(samples, soft_blending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention_layer_groups = [\n",
    "    [2 * x + 1] for x in range(len(model_proxy.decoder_layers))\n",
    "]\n",
    "df = run_ablation_study(dataset, self_attention_layer_groups)\n",
    "df.to_pickle(RESULTS_DIR / \"individual_soft.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "#### Comparing `n - 1` self-attention layers (Soft-blending self-attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention_layer_groups = [\n",
    "    [2 * y + 1 for y in range(len(model_proxy.decoder_layers)) if x != y]\n",
    "    for x in range(len(model_proxy.decoder_layers))\n",
    "]\n",
    "df = run_ablation_study(dataset, self_attention_layer_groups)\n",
    "df.to_pickle(RESULTS_DIR / \"leave_one_out_soft.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "#### Comparing incremental groups of self-attention layers (Soft-blending self-attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention_layer_groups = [[2 * y + 1 for y in range(0, x)] for x in range(1, 49)]\n",
    "df = run_ablation_study(dataset, self_attention_layer_groups)\n",
    "df.to_pickle(RESULTS_DIR / \"incremental_soft.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### Cherrypicking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(prompts: list[str], controller: BaseController):\n",
    "    audio_values = run_and_display(prompts, controller, save_results=False)\n",
    "\n",
    "    audio_audio_similarity, text_audio_similarity = cosine_similarity(\n",
    "        prompts[1], audio_values\n",
    "    )\n",
    "\n",
    "    print(f\"T2A Similarity:{text_audio_similarity:.3f}\")\n",
    "    print(f\"A2A Similarity: {audio_audio_similarity:.3f}\")\n",
    "\n",
    "    return audio_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts, indices = get_ignore_indices(\n",
    "    [\n",
    "        \"accoustic guitar solo\",\n",
    "        \"<IGNORE> guitar solo\",\n",
    "    ]\n",
    ")\n",
    "controller = SelfAttentionLerpControllerModifier(IgnoreWordController(indices))\n",
    "\n",
    "audio_values = run(prompts, controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[0], rate=model_proxy.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[1], rate=model_proxy.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "### Replacement edit with Prompt-to-Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"pop song with guitar and drums\",\n",
    "    \"pop song with synth and drums\",\n",
    "]\n",
    "# controller = DecoderLayerControllerModifier(ReplaceWordController(3, 0.3), set([2 * i for i in range(0, 49)] + [1]))\n",
    "# controller = DecoderLayerControllerModifier(ReplaceWordController(3, 0.3), {0,1,2,3,4,5,6,7,8,9,10,11,12,14,15,16,17,18,19,20,21,24,33,34})\n",
    "controller = AttentionLerpControllerModifier(\n",
    "    ReplaceWordController(get_replacement_indices(prompts, \"guitar\", \"synth\"), 1)\n",
    ")\n",
    "# controller = ReplaceWordController(3, 0.3)\n",
    "audio_values = run_and_display(prompts, controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[0], rate=model_proxy.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[1], rate=model_proxy.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "### Refinement edit with Prompt-to-Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"heavy guitar solo\",\n",
    "    \"heavy guitar and drums solo\",\n",
    "]\n",
    "controller = AttentionLerpControllerModifier(\n",
    "    RefineController(get_refine_word_indices(prompts), 1)\n",
    ")\n",
    "audio_values = run_and_display(prompts, controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[0], rate=model_proxy.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[1], rate=model_proxy.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "### Reweight edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"heavy guitar solo\",\n",
    "    \"heavy guitar solo\",\n",
    "]\n",
    "controller = AttentionCutoffControllerModifier(\n",
    "    ReweightWordController(get_reweight_word_indices(prompts, \"heavy\"), 2)\n",
    ")\n",
    "audio_values = run_and_display(prompts, controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[0], rate=model_proxy.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[1], rate=model_proxy.sampling_rate)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "editgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
